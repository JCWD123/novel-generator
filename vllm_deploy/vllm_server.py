#!/usr/bin/env python3
"""
vLLM æœåŠ¡å¯åŠ¨è„šæœ¬

ç”¨äºåœ¨ Docker å®¹å™¨å†…å¯åŠ¨ vLLM OpenAI å…¼å®¹ API æœåŠ¡å™¨
"""
import os
import sys
import subprocess

from config import (
    MODEL_PATH,
    VLLM_HOST,
    VLLM_PORT,
    MODEL_NAME,
    TENSOR_PARALLEL_SIZE,
    GPU_MEMORY_UTILIZATION,
    MAX_MODEL_LEN,
    DTYPE,
    print_config
)


def check_model_path(model_path: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    from pathlib import Path
    
    path = Path(model_path)
    if not path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    config_file = path / "config.json"
    if not config_file.exists():
        print(f"âŒ æœªæ‰¾åˆ° config.json: {model_path}")
        return False
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
    has_weights = any(
        path.glob("*.safetensors")
    ) or any(
        path.glob("*.bin")
    )
    
    if not has_weights:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡å‹æƒé‡æ–‡ä»¶: {model_path}")
        return False
    
    print(f"âœ… æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡: {model_path}")
    return True


def start_vllm_server():
    """å¯åŠ¨ vLLM æœåŠ¡å™¨"""
    
    print("\n" + "=" * 60)
    print("ğŸš€ vLLM OpenAI å…¼å®¹æœåŠ¡å™¨")
    print("=" * 60)
    
    # æ‰“å°é…ç½®
    print_config()
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    model_path = os.getenv("MODEL_PATH", MODEL_PATH)
    if not check_model_path(model_path):
        print("\nè¯·ç¡®ä¿æ¨¡å‹å·²æ­£ç¡®æŒ‚è½½åˆ°å®¹å™¨ä¸­")
        sys.exit(1)
    
    # æ„å»ºå¯åŠ¨å‘½ä»¤
    cmd = [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", model_path,
        "--host", os.getenv("VLLM_HOST", "0.0.0.0"),
        "--port", str(os.getenv("VLLM_PORT", VLLM_PORT)),
        "--tensor-parallel-size", str(os.getenv("TENSOR_PARALLEL_SIZE", TENSOR_PARALLEL_SIZE)),
        "--gpu-memory-utilization", str(os.getenv("GPU_MEMORY_UTILIZATION", GPU_MEMORY_UTILIZATION)),
        "--max-model-len", str(os.getenv("MAX_MODEL_LEN", MAX_MODEL_LEN)),
        "--dtype", os.getenv("DTYPE", DTYPE),
        "--served-model-name", os.getenv("MODEL_NAME", MODEL_NAME),
        "--trust-remote-code",
    ]
    
    # æ˜¯å¦ä½¿ç”¨ AWQ é‡åŒ–
    quantization = os.getenv("QUANTIZATION", "")
    if quantization:
        cmd.extend(["--quantization", quantization])
    
    # æ˜¯å¦å¼ºåˆ¶ eager æ¨¡å¼
    if os.getenv("ENFORCE_EAGER", "true").lower() == "true":
        cmd.append("--enforce-eager")
    
    print(f"\nğŸ“‹ å¯åŠ¨å‘½ä»¤:\n{' '.join(cmd)}\n")
    print("=" * 60 + "\n")
    
    try:
        # å¯åŠ¨æœåŠ¡å™¨
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æœåŠ¡å™¨å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    start_vllm_server()

