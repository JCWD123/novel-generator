# ğŸ“š AI å°è¯´ç”Ÿæˆå™¨ - vLLM Docker éƒ¨ç½²

åŸºäº **vLLM + Streamlit + LangChain** çš„å°è¯´ç”Ÿæˆå™¨ï¼Œæ”¯æŒ Docker ä¸€é”®éƒ¨ç½²ã€‚

## ğŸ¯ ç‰¹æ€§

- âœ… **vLLM æ¨ç†æœåŠ¡**: é«˜æ€§èƒ½ LLM æ¨ç†ï¼Œæ”¯æŒ AWQ é‡åŒ–
- âœ… **Streamlit å‰ç«¯**: ç°ä»£åŒ– Web ç•Œé¢ï¼Œæ”¯æŒæµå¼ç”Ÿæˆ
- âœ… **LangChain é›†æˆ**: å®Œæ•´çš„å†å²å¯¹è¯ç®¡ç†
- âœ… **Docker éƒ¨ç½²**: ä¸€æ¬¡æ„å»ºï¼Œå¤šå¤„éƒ¨ç½²
- âœ… **å›½å†…é•œåƒåŠ é€Ÿ**: ä½¿ç”¨é˜¿é‡Œäº‘ã€æ¸…åæºåŠ é€Ÿä¸‹è½½

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

### ç¨³å®šç»„åˆ (æ¨è)

| ç»„ä»¶ | ç‰ˆæœ¬ |
|------|------|
| CUDA Runtime | 12.1 |
| NVIDIA Driver | â‰¥ 535.xx |
| Python | 3.10 |
| PyTorch | 2.1.2 + cu121 |
| Triton | 2.1.0 |
| vLLM | 0.3.3 |
| OS | Ubuntu 22.04 |
| Docker | nvidia-container-toolkit |

### ç¡¬ä»¶è¦æ±‚

- **GPU**: NVIDIA GPU (æ˜¾å­˜ â‰¥ 8GBï¼Œæ¨è â‰¥ 16GB)
- **å†…å­˜**: â‰¥ 16GB RAM
- **å­˜å‚¨**: â‰¥ 50GB (é•œåƒ + æ¨¡å‹)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æœ¬åœ°æ„å»ºé•œåƒ

```bash
cd vllm_deploy

# æ„å»ºé•œåƒ
chmod +x build.sh
./build.sh
```

### 2. å¯¼å‡ºé•œåƒ

```bash
# å¯¼å‡ºä¸º tar æ–‡ä»¶
chmod +x export.sh
./export.sh

# ç”Ÿæˆ: novel-images.tar
```

### 3. ä¼ è¾“åˆ°æœåŠ¡å™¨

```bash
# ä½¿ç”¨ scp ä¼ è¾“
scp novel-images.tar user@server:/path/to/deploy/

# æˆ–ä½¿ç”¨å…¶ä»–æ–¹å¼ (rsync, sftp ç­‰)
```

### 4. æœåŠ¡å™¨éƒ¨ç½²

```bash
# SSH ç™»å½•æœåŠ¡å™¨
ssh user@server

cd /path/to/deploy/

# å¤åˆ¶éƒ¨ç½²æ–‡ä»¶
# - docker-compose.yml
# - deploy.sh

# è¿è¡Œéƒ¨ç½²è„šæœ¬
chmod +x deploy.sh
./deploy.sh novel-images.tar
```

### 5. å‡†å¤‡æ¨¡å‹

å°†æ¨¡å‹æ–‡ä»¶æ”¾ç½®åˆ° `models/` ç›®å½•ï¼š

```
models/
â””â”€â”€ DeepSeek-R1-7B-AWQ/
    â”œâ”€â”€ config.json
    â”œâ”€â”€ model-00001-of-00002.safetensors
    â”œâ”€â”€ model-00002-of-00002.safetensors
    â”œâ”€â”€ tokenizer.json
    â””â”€â”€ tokenizer_config.json
```

### 6. å¯åŠ¨æœåŠ¡

```bash
docker compose up -d
```

## ğŸ“ ç›®å½•ç»“æ„

```
vllm_deploy/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ langchain_history.py   # LangChain å†å²ç®¡ç†
â”œâ”€â”€ vllm_client.py         # vLLM å®¢æˆ·ç«¯
â”œâ”€â”€ streamlit_app.py       # Streamlit å‰ç«¯
â”œâ”€â”€ vllm_server.py         # vLLM æœåŠ¡å¯åŠ¨è„šæœ¬
â”œâ”€â”€ requirements.txt       # Python ä¾èµ–
â”œâ”€â”€ Dockerfile.vllm        # vLLM æœåŠ¡é•œåƒ
â”œâ”€â”€ Dockerfile.streamlit   # Streamlit é•œåƒ
â”œâ”€â”€ docker-compose.yml     # Docker Compose é…ç½®
â”œâ”€â”€ streamlit_config.toml  # Streamlit é…ç½®
â”œâ”€â”€ build.sh               # æ„å»ºè„šæœ¬
â”œâ”€â”€ export.sh              # å¯¼å‡ºè„šæœ¬
â”œâ”€â”€ deploy.sh              # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ chat_history/          # å†å²è®°å½•å­˜å‚¨
â””â”€â”€ README.md              # æœ¬æ–‡æ¡£
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡

| å˜é‡ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `MODEL_PATH` | `/models/DeepSeek-R1-7B-AWQ` | æ¨¡å‹è·¯å¾„ |
| `MODEL_NAME` | `deepseek-r1` | API æ¨¡å‹åç§° |
| `VLLM_PORT` | `8000` | vLLM æœåŠ¡ç«¯å£ |
| `STREAMLIT_PORT` | `8501` | Streamlit ç«¯å£ |
| `TENSOR_PARALLEL_SIZE` | `1` | GPU æ•°é‡ |
| `GPU_MEMORY_UTILIZATION` | `0.9` | æ˜¾å­˜åˆ©ç”¨ç‡ |
| `MAX_MODEL_LEN` | `4096` | æœ€å¤§åºåˆ—é•¿åº¦ |
| `QUANTIZATION` | `awq` | é‡åŒ–æ–¹å¼ |

### ä¿®æ”¹é…ç½®

ç¼–è¾‘ `docker-compose.yml` ä¸­çš„ `environment` éƒ¨åˆ†ï¼š

```yaml
environment:
  - MODEL_PATH=/models/your-model
  - TENSOR_PARALLEL_SIZE=2  # ä½¿ç”¨ 2 å¼  GPU
  - GPU_MEMORY_UTILIZATION=0.85
```

## ğŸ”§ å¸¸ç”¨å‘½ä»¤

### Docker Compose

```bash
# å¯åŠ¨æœåŠ¡
docker compose up -d

# æŸ¥çœ‹çŠ¶æ€
docker compose ps

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f
docker compose logs -f vllm-server
docker compose logs -f streamlit

# åœæ­¢æœåŠ¡
docker compose down

# é‡å¯æœåŠ¡
docker compose restart
```

### é•œåƒç®¡ç†

```bash
# æŸ¥çœ‹é•œåƒ
docker images | grep novel

# åˆ é™¤é•œåƒ
docker rmi novel-vllm:latest novel-streamlit:latest

# é‡æ–°æ„å»º
docker compose build --no-cache
```

## ğŸ› é—®é¢˜æ’æŸ¥

### 1. GPU ä¸å¯ç”¨

```bash
# æ£€æŸ¥ NVIDIA é©±åŠ¨
nvidia-smi

# æ£€æŸ¥ nvidia-container-toolkit
docker info | grep -i nvidia

# æµ‹è¯• GPU Docker
docker run --rm --gpus all nvidia/cuda:12.1.1-base-ubuntu22.04 nvidia-smi
```

### 2. vLLM æœåŠ¡å¯åŠ¨å¤±è´¥

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker compose logs vllm-server

# å¸¸è§åŸå› :
# - æ¨¡å‹è·¯å¾„é”™è¯¯
# - æ˜¾å­˜ä¸è¶³
# - CUDA ç‰ˆæœ¬ä¸åŒ¹é…
```

### 3. è¿æ¥ vLLM å¤±è´¥

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# æ£€æŸ¥æ¨¡å‹åˆ—è¡¨
curl http://localhost:8000/v1/models
```

### 4. æ˜¾å­˜ä¸è¶³

- é™ä½ `GPU_MEMORY_UTILIZATION` (å¦‚ 0.7)
- ä½¿ç”¨é‡åŒ–æ¨¡å‹ (AWQ)
- å‡å°‘ `MAX_MODEL_LEN`

## ğŸŒ è®¿é—®åœ°å€

éƒ¨ç½²å®Œæˆåè®¿é—®:

- **Streamlit å‰ç«¯**: http://localhost:8501
- **vLLM API**: http://localhost:8000
  - æ¨¡å‹åˆ—è¡¨: http://localhost:8000/v1/models
  - å¥åº·æ£€æŸ¥: http://localhost:8000/health

## ğŸ“ API ä½¿ç”¨ç¤ºä¾‹

vLLM æä¾› OpenAI å…¼å®¹ API:

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)

response = client.chat.completions.create(
    model="deepseek-r1",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°è¯´ä½œå®¶"},
        {"role": "user", "content": "å†™ä¸€ä¸ªå¼€å¤´"}
    ],
    max_tokens=1024,
    temperature=0.8
)

print(response.choices[0].message.content)
```

## ğŸ“„ License

MIT License

