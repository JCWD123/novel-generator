#
# vLLM 服务锁定依赖（GPU / CUDA 12.1）
# 避免“裸 pip install vllm”，统一从锁文件安装
# torch / torchvision / vllm 由 Dockerfile 通过 wheel 方式固定：
#   torch==2.4.1+cu121
#   torchvision==0.19.1
#   vllm==0.6.3.post1
# 这里锁定其余依赖，避免冲突。
#
# transformers 4.45.x 收录 qwen2/DeepSeek-R1 配置
transformers==4.45.2
accelerate==0.30.1
sentencepiece==0.2.0
protobuf==5.28.3
# vLLM 0.6.x 依赖 outlines，保持兼容范围
outlines==0.0.46
numpy==1.26.4
requests==2.32.3
# 可选：提示输出更清晰
colorama==0.4.6
# outlines 0.0.46 依赖的可选模块
pyairports==2.1.0

