#!/usr/bin/env python3
"""
æ¨¡å‹é‡åŒ–è„šæœ¬

æ”¯æŒå¤šç§é‡åŒ–æ–¹å¼ï¼š
1. BitsAndBytes 4-bit é‡åŒ–ï¼ˆæ¨èï¼Œç›´æ¥åœ¨ vLLM ä¸­ä½¿ç”¨ï¼‰
2. GPTQ é‡åŒ–
3. ä½¿ç”¨é¢„é‡åŒ–æ¨¡å‹

ç”±äº autoawq åº“å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œæ¨èç›´æ¥ä½¿ç”¨ vLLM çš„è¿è¡Œæ—¶é‡åŒ–åŠŸèƒ½ã€‚

ç”¨æ³•ï¼š
    # æ–¹å¼ä¸€ï¼ˆæ¨èï¼‰ï¼šç›´æ¥åœ¨ vLLM ä¸­å¯ç”¨é‡åŒ–ï¼Œæ— éœ€é¢„å¤„ç†
    python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --quantization bnb
    
    # æ–¹å¼äºŒï¼šä¸‹è½½é¢„é‡åŒ–æ¨¡å‹
    python awq_quantization.py --download-quantized deepseek-r1-7b
    
    # æ–¹å¼ä¸‰ï¼šä½¿ç”¨ GPTQ é‡åŒ–ï¼ˆéœ€è¦ auto-gptqï¼‰
    python awq_quantization.py --gptq --model-path /path/to/model --output /path/to/output

æ³¨æ„ï¼š
    - vLLM >= 0.6.0 æ”¯æŒ BitsAndBytes è¿è¡Œæ—¶ 4-bit é‡åŒ–
    - æ— éœ€é¢„å…ˆé‡åŒ–æ¨¡å‹ï¼Œç›´æ¥åœ¨å¯åŠ¨æ—¶æ·»åŠ  --quantization bnb å³å¯
    - é¢„é‡åŒ–æ¨¡å‹å¯ä»¥åŠ å¿«å¯åŠ¨é€Ÿåº¦ï¼Œä½†çµæ´»æ€§è¾ƒä½
"""

import argparse
import os
import sys
import subprocess
from pathlib import Path
from typing import Optional


# é¢„é‡åŒ–æ¨¡å‹åˆ—è¡¨ï¼ˆHuggingFace ä¸Šå¯ç”¨çš„é¢„é‡åŒ–ç‰ˆæœ¬ï¼‰
PRE_QUANTIZED_MODELS = {
    # DeepSeek-R1 AWQ ç‰ˆæœ¬
    "deepseek-r1-7b-awq": "TheBloke/DeepSeek-R1-Distill-Qwen-7B-AWQ",
    "deepseek-r1-14b-awq": "TheBloke/DeepSeek-R1-Distill-Qwen-14B-AWQ",
    "deepseek-r1-32b-awq": "TheBloke/DeepSeek-R1-Distill-Qwen-32B-AWQ",
    
    # DeepSeek-R1 GPTQ ç‰ˆæœ¬
    "deepseek-r1-7b-gptq": "TheBloke/DeepSeek-R1-Distill-Qwen-7B-GPTQ",
    "deepseek-r1-14b-gptq": "TheBloke/DeepSeek-R1-Distill-Qwen-14B-GPTQ",
    
    # å…¶ä»–å¸¸ç”¨æ¨¡å‹
    "qwen2-7b-awq": "Qwen/Qwen2-7B-Instruct-AWQ",
    "llama3-8b-awq": "casperhansen/llama-3-8b-instruct-awq",
}


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...\n")
    
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨")
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        print("âŒ Transformers æœªå®‰è£…")
        return False
    
    # æ£€æŸ¥ bitsandbytesï¼ˆå¯é€‰ï¼‰
    try:
        import bitsandbytes
        print(f"âœ… BitsAndBytes ç‰ˆæœ¬: {bitsandbytes.__version__}")
    except ImportError:
        print("âš ï¸ BitsAndBytes æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äº 4-bit é‡åŒ–ï¼‰")
    
    return True


def download_model(model_name: str, use_mirror: bool = False) -> bool:
    """ä¸‹è½½æ¨¡å‹"""
    print(f"\n{'='*70}")
    print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_name}")
    print(f"{'='*70}")
    
    env = os.environ.copy()
    
    if use_mirror:
        env["HF_ENDPOINT"] = "https://hf-mirror.com"
        print(f"ğŸŒ ä½¿ç”¨é•œåƒ: https://hf-mirror.com")
    
    # å°è¯•å¯ç”¨ hf_transfer
    try:
        import hf_transfer
        env["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
        print(f"âš¡ å¯ç”¨ hf_transfer åŠ é€Ÿ")
    except ImportError:
        print(f"ğŸ“¡ ä½¿ç”¨æ™®é€šä¸‹è½½æ¨¡å¼")
    
    cmd = [
        sys.executable, "-m", "huggingface_hub.commands.huggingface_cli",
        "download", model_name
    ]
    
    try:
        process = subprocess.run(cmd, env=env)
        return process.returncode == 0
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        return False


def list_quantized_models():
    """åˆ—å‡ºå¯ç”¨çš„é¢„é‡åŒ–æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ“¦ å¯ç”¨çš„é¢„é‡åŒ–æ¨¡å‹")
    print("="*70)
    
    print("\nğŸ”¸ DeepSeek-R1 ç³»åˆ— (AWQ 4-bit):")
    for key, model in PRE_QUANTIZED_MODELS.items():
        if "deepseek" in key and "awq" in key:
            print(f"  {key}: {model}")
    
    print("\nğŸ”¸ DeepSeek-R1 ç³»åˆ— (GPTQ 4-bit):")
    for key, model in PRE_QUANTIZED_MODELS.items():
        if "deepseek" in key and "gptq" in key:
            print(f"  {key}: {model}")
    
    print("\nğŸ”¸ å…¶ä»–æ¨¡å‹:")
    for key, model in PRE_QUANTIZED_MODELS.items():
        if "deepseek" not in key:
            print(f"  {key}: {model}")
    
    print("\n" + "="*70)
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("  python awq_quantization.py --download-quantized deepseek-r1-7b-awq --mirror")
    print("\nğŸ’¡ æˆ–è€…ç›´æ¥ä½¿ç”¨ vLLM è¿è¡Œæ—¶é‡åŒ–ï¼ˆæ¨èï¼‰:")
    print("  python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --quantization bnb")
    print("="*70)


def convert_to_bnb_4bit(
    model_path: str,
    output_path: str,
    compute_dtype: str = "bfloat16"
):
    """
    å°†æ¨¡å‹è½¬æ¢ä¸º BitsAndBytes 4-bit æ ¼å¼
    
    æ³¨æ„ï¼šè¿™ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„æ¨¡å‹ç›®å½•ï¼Œä½† vLLM æ›´æ¨èç›´æ¥ä½¿ç”¨è¿è¡Œæ—¶é‡åŒ–
    """
    print(f"\n{'='*70}")
    print(f"ğŸ”§ BitsAndBytes 4-bit è½¬æ¢")
    print(f"{'='*70}")
    print(f"ğŸ“‚ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
    print(f"{'='*70}\n")
    
    try:
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
        
        # é…ç½®é‡åŒ–
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=getattr(torch, compute_dtype),
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        
        print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )
        
        print("ğŸ“¥ åŠ è½½ Tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        print("ğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
        Path(output_path).mkdir(parents=True, exist_ok=True)
        model.save_pretrained(output_path)
        tokenizer.save_pretrained(output_path)
        
        print(f"\nâœ… è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“‚ è¾“å‡ºè·¯å¾„: {output_path}")
        
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
        print("è¯·å®‰è£…: pip install bitsandbytes accelerate")
        return False
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False
    
    return True


def main():
    parser = argparse.ArgumentParser(
        description="æ¨¡å‹é‡åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ æ¨èæ–¹å¼ï¼š

  ğŸ”¹ æ–¹å¼ä¸€ï¼ˆæœ€ç®€å•ï¼‰ï¼šç›´æ¥ä½¿ç”¨ vLLM è¿è¡Œæ—¶é‡åŒ–
     python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --quantization bnb
     
  ğŸ”¹ æ–¹å¼äºŒï¼šä¸‹è½½é¢„é‡åŒ–æ¨¡å‹
     python awq_quantization.py --download-quantized deepseek-r1-7b-awq --mirror
     python start_vllm_server.py --model TheBloke/DeepSeek-R1-Distill-Qwen-7B-AWQ --quantization awq

  ğŸ”¹ æ–¹å¼ä¸‰ï¼šåŒå¡å¼ é‡å¹¶è¡Œ + é‡åŒ–
     python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \\
         --quantization bnb --tensor-parallel 2

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ è¯´æ˜ï¼š
  - vLLM >= 0.6.0 æ”¯æŒ BitsAndBytes è¿è¡Œæ—¶ 4-bit é‡åŒ–
  - è¿è¡Œæ—¶é‡åŒ–æ— éœ€é¢„å¤„ç†ï¼Œç›´æ¥æ·»åŠ  --quantization bnb å‚æ•°
  - é¢„é‡åŒ–æ¨¡å‹å¯åŠ¨æ›´å¿«ï¼Œä½†éœ€è¦é¢å¤–ä¸‹è½½
  - å¼ é‡å¹¶è¡Œå¯å°†æ¨¡å‹åˆ†å¸ƒåˆ°å¤šå¼  GPU
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
    )
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºå¯ç”¨çš„é¢„é‡åŒ–æ¨¡å‹"
    )
    
    # ä¸‹è½½é¢„é‡åŒ–æ¨¡å‹
    parser.add_argument(
        "--download-quantized",
        type=str,
        metavar="MODEL_KEY",
        help="ä¸‹è½½é¢„é‡åŒ–æ¨¡å‹ï¼ˆå¦‚ deepseek-r1-7b-awqï¼‰"
    )
    
    # BNB è½¬æ¢
    parser.add_argument(
        "--convert-bnb",
        action="store_true",
        help="å°†æ¨¡å‹è½¬æ¢ä¸º BitsAndBytes 4-bit æ ¼å¼"
    )
    parser.add_argument(
        "--model-path",
        type=str,
        help="æºæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--output",
        type=str,
        help="è¾“å‡ºè·¯å¾„"
    )
    
    # é€šç”¨é€‰é¡¹
    parser.add_argument(
        "--mirror",
        action="store_true",
        help="ä½¿ç”¨ HuggingFace é•œåƒ (hf-mirror.com)"
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="åªæ£€æŸ¥ä¾èµ–"
    )
    
    args = parser.parse_args()
    
    # åˆ—å‡ºæ¨¡å‹
    if args.list:
        list_quantized_models()
        return 0
    
    # æ£€æŸ¥ä¾èµ–
    if args.check:
        check_dependencies()
        return 0
    
    # ä¸‹è½½é¢„é‡åŒ–æ¨¡å‹
    if args.download_quantized:
        model_key = args.download_quantized
        
        if model_key in PRE_QUANTIZED_MODELS:
            model_name = PRE_QUANTIZED_MODELS[model_key]
        else:
            # å°è¯•ä½œä¸ºå®Œæ•´æ¨¡å‹åä½¿ç”¨
            model_name = model_key
        
        print(f"\nä¸‹è½½é¢„é‡åŒ–æ¨¡å‹: {model_name}")
        
        if download_model(model_name, use_mirror=args.mirror):
            print(f"\nâœ… ä¸‹è½½å®Œæˆ!")
            print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            
            if "awq" in model_key.lower() or "awq" in model_name.lower():
                print(f"  python start_vllm_server.py --model {model_name} --quantization awq")
            elif "gptq" in model_key.lower() or "gptq" in model_name.lower():
                print(f"  python start_vllm_server.py --model {model_name} --quantization gptq")
            else:
                print(f"  python start_vllm_server.py --model {model_name}")
        else:
            print(f"\nâŒ ä¸‹è½½å¤±è´¥")
            return 1
        
        return 0
    
    # BNB è½¬æ¢
    if args.convert_bnb:
        if not args.model_path or not args.output:
            print("âŒ è¯·æŒ‡å®š --model-path å’Œ --output")
            return 1
        
        check_dependencies()
        
        if convert_to_bnb_4bit(args.model_path, args.output):
            print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            print(f"  python start_vllm_server.py --model {args.output} --quantization bnb")
        else:
            return 1
        
        return 0
    
    # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
    print("\n" + "="*70)
    print("ğŸ“š æ¨¡å‹é‡åŒ–æŒ‡å—")
    print("="*70)
    
    print("""
ğŸ’¡ æ¨èæ–¹å¼ï¼šç›´æ¥ä½¿ç”¨ vLLM è¿è¡Œæ—¶é‡åŒ–ï¼ˆæœ€ç®€å•ï¼‰

  # å•å¡ + BNB 4-bit é‡åŒ–
  python start_vllm_server.py \\
      --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\
      --quantization bnb

  # åŒå¡å¼ é‡å¹¶è¡Œ + é‡åŒ–ï¼ˆé€‚åˆ 70B æ¨¡å‹ï¼‰
  python start_vllm_server.py \\
      --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \\
      --quantization bnb \\
      --tensor-parallel 2

  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
  python start_vllm_server.py \\
      --model /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B \\
      --quantization bnb \\
      --tensor-parallel 2
""")
    
    print("="*70)
    print("\nè¿è¡Œ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
