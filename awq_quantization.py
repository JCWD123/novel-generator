#!/usr/bin/env python3
"""
AWQ 4-bit é‡åŒ–è„šæœ¬

å°† DeepSeek-R1 è’¸é¦ç‰ˆæ¨¡å‹é‡åŒ–ä¸º AWQ 4-bit æ ¼å¼ï¼Œå¤§å¹…å‡å°‘æ˜¾å­˜å ç”¨ã€‚

ç”¨æ³•ï¼š
    # é‡åŒ– 70B æ¨¡å‹ï¼ˆéœ€è¦è¾ƒå¤§å†…å­˜ï¼‰
    python awq_quantization.py \
        --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B \
        --quant_path /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ \
        --max_calib_seq_len 2048

    # é‡åŒ– 7B æ¨¡å‹
    python awq_quantization.py \
        --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Qwen-7B \
        --quant_path /home/user/models/DeepSeek-R1-Distill-Qwen-7B-AWQ
        
    # ä½¿ç”¨è‡ªå®šä¹‰æ ¡å‡†æ•°æ®
    python awq_quantization.py \
        --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B \
        --quant_path /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ \
        --calib_data pileval \
        --max_calib_seq_len 2048 \
        --max_calib_samples 512

é‡åŒ–é…ç½®è¯´æ˜ï¼š
    - w_bit: æƒé‡ä½æ•°ï¼Œ4è¡¨ç¤º4-bité‡åŒ–
    - q_group_size: é‡åŒ–åˆ†ç»„å¤§å°ï¼Œ64æ˜¯å¸¸ç”¨å€¼
    - zero_point: æ˜¯å¦ä½¿ç”¨é›¶ç‚¹é‡åŒ–
    - version: GEMMè¡¨ç¤ºä½¿ç”¨çŸ©é˜µä¹˜æ³•ä¼˜åŒ–çš„å†…æ ¸
"""

import argparse
import os
import sys
from pathlib import Path


def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    missing = []
    
    try:
        import awq
        print(f"âœ… AutoAWQ ç‰ˆæœ¬: {awq.__version__}")
    except ImportError:
        missing.append("autoawq")
    
    try:
        import transformers
        print(f"âœ… Transformers ç‰ˆæœ¬: {transformers.__version__}")
    except ImportError:
        missing.append("transformers")
    
    try:
        import torch
        print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"âœ… æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œé‡åŒ–å°†åœ¨ CPU ä¸Šè¿›è¡Œï¼ˆéå¸¸æ…¢ï¼‰")
    except ImportError:
        missing.append("torch")
    
    if missing:
        print(f"\nâŒ ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install " + " ".join(missing))
        sys.exit(1)
    
    return True


def quantize_model(
    model_path: str,
    quant_path: str,
    calib_data: str = "pileval",
    max_calib_seq_len: int = 2048,
    max_calib_samples: int = 512,
    w_bit: int = 4,
    q_group_size: int = 64,
    zero_point: bool = True,
    version: str = "GEMM"
):
    """
    æ‰§è¡Œ AWQ é‡åŒ–
    
    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„
        quant_path: é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„
        calib_data: æ ¡å‡†æ•°æ®é›†åç§°æˆ–è·¯å¾„
        max_calib_seq_len: æ ¡å‡†åºåˆ—æœ€å¤§é•¿åº¦
        max_calib_samples: æ ¡å‡†æ ·æœ¬æ•°é‡
        w_bit: æƒé‡ä½æ•° (4)
        q_group_size: é‡åŒ–åˆ†ç»„å¤§å° (64/128)
        zero_point: æ˜¯å¦ä½¿ç”¨é›¶ç‚¹
        version: é‡åŒ–ç‰ˆæœ¬ (GEMM/GEMV)
    """
    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer, AwqConfig
    
    print(f"\n{'='*70}")
    print(f"ğŸ”§ AWQ 4-bit é‡åŒ–")
    print(f"{'='*70}")
    print(f"ğŸ“‚ æºæ¨¡å‹: {model_path}")
    print(f"ğŸ“‚ ç›®æ ‡è·¯å¾„: {quant_path}")
    print(f"ğŸ“Š æ ¡å‡†æ•°æ®: {calib_data}")
    print(f"ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {max_calib_seq_len}")
    print(f"ğŸ“ˆ æ ¡å‡†æ ·æœ¬æ•°: {max_calib_samples}")
    print(f"{'='*70}")
    print(f"âš™ï¸ é‡åŒ–é…ç½®:")
    print(f"   â”œâ”€ æƒé‡ä½æ•°: {w_bit}-bit")
    print(f"   â”œâ”€ åˆ†ç»„å¤§å°: {q_group_size}")
    print(f"   â”œâ”€ é›¶ç‚¹é‡åŒ–: {'æ˜¯' if zero_point else 'å¦'}")
    print(f"   â””â”€ ç‰ˆæœ¬: {version}")
    print(f"{'='*70}\n")
    
    # æ£€æŸ¥æºæ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(quant_path).mkdir(parents=True, exist_ok=True)
    
    # é‡åŒ–é…ç½®
    quant_config = {
        "zero_point": zero_point,
        "q_group_size": q_group_size,
        "w_bit": w_bit,
        "version": version
    }
    
    print("ğŸ“¥ åŠ è½½æ¨¡å‹...")
    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        safetensors=True,  # ä¼˜å…ˆä½¿ç”¨ safetensors æ ¼å¼
    )
    
    print("ğŸ“¥ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    print(f"\nğŸ”„ å¼€å§‹é‡åŒ–ï¼ˆè¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰...")
    print(f"   ä½¿ç”¨æ ¡å‡†æ•°æ®: {calib_data}")
    
    # æ‰§è¡Œé‡åŒ–
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calib_data,
        max_calib_seq_len=max_calib_seq_len,
        max_calib_samples=max_calib_samples,
    )
    
    print("\nğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
    
    # åˆ›å»ºé‡åŒ–é…ç½®
    quantization_config = AwqConfig(
        bits=quant_config["w_bit"],
        group_size=quant_config["q_group_size"],
        zero_point=quant_config["zero_point"],
        version=quant_config["version"].lower(),
    ).to_dict()
    
    # æ›´æ–°æ¨¡å‹é…ç½®
    model.model.config.quantization_config = quantization_config
    
    # ä¿å­˜æ¨¡å‹å’Œ tokenizer
    model.save_quantized(quant_path)
    tokenizer.save_pretrained(quant_path)
    
    # è®¡ç®—å‹ç¼©åå¤§å°
    quant_size = sum(f.stat().st_size for f in Path(quant_path).rglob("*") if f.is_file())
    quant_size_gb = quant_size / (1024**3)
    
    print(f"\n{'='*70}")
    print(f"âœ… é‡åŒ–å®Œæˆ!")
    print(f"{'='*70}")
    print(f"ğŸ“‚ ä¿å­˜è·¯å¾„: {quant_path}")
    print(f"ğŸ“¦ æ¨¡å‹å¤§å°: {quant_size_gb:.2f} GB")
    print(f"{'='*70}")
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   python start_vllm_server.py --model {quant_path} --quantization awq")
    print(f"{'='*70}\n")


def main():
    parser = argparse.ArgumentParser(
        description="AWQ 4-bit æ¨¡å‹é‡åŒ–å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # é‡åŒ– DeepSeek-R1 70B è’¸é¦ç‰ˆ
  python awq_quantization.py \\
      --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B \\
      --quant_path /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ
      
  # é‡åŒ– 7B æ¨¡å‹ï¼ˆé€‚åˆæµ‹è¯•ï¼‰
  python awq_quantization.py \\
      --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Qwen-7B \\
      --quant_path /home/user/models/DeepSeek-R1-Distill-Qwen-7B-AWQ \\
      --max_calib_seq_len 1024

æ ¡å‡†æ•°æ®é›†é€‰é¡¹:
  - pileval (é»˜è®¤): AutoAWQ å†…ç½®çš„ WikiText æ•°æ®é›†
  - wikitext: HuggingFace wikitext æ•°æ®é›†
  - è‡ªå®šä¹‰ HuggingFace æ•°æ®é›†è·¯å¾„
        """
    )
    
    parser.add_argument(
        "--model_path",
        type=str,
        default="/home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B",
        help="åŸå§‹æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--quant_path",
        type=str,
        default="/home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ",
        help="é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„"
    )
    parser.add_argument(
        "--calib_data",
        type=str,
        default="pileval",
        help="æ ¡å‡†æ•°æ®é›† (pileval/wikitext/è‡ªå®šä¹‰HFæ•°æ®é›†)"
    )
    parser.add_argument(
        "--max_calib_seq_len",
        type=int,
        default=2048,
        help="æ ¡å‡†åºåˆ—æœ€å¤§é•¿åº¦ (é»˜è®¤: 2048)"
    )
    parser.add_argument(
        "--max_calib_samples",
        type=int,
        default=512,
        help="æ ¡å‡†æ ·æœ¬æ•°é‡ (é»˜è®¤: 512)"
    )
    parser.add_argument(
        "--w_bit",
        type=int,
        default=4,
        choices=[4, 8],
        help="é‡åŒ–ä½æ•° (é»˜è®¤: 4)"
    )
    parser.add_argument(
        "--q_group_size",
        type=int,
        default=64,
        choices=[32, 64, 128],
        help="é‡åŒ–åˆ†ç»„å¤§å° (é»˜è®¤: 64)"
    )
    parser.add_argument(
        "--check-only",
        action="store_true",
        help="åªæ£€æŸ¥ä¾èµ–ï¼Œä¸æ‰§è¡Œé‡åŒ–"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    print("ğŸ” æ£€æŸ¥ä¾èµ–...\n")
    check_dependencies()
    
    if args.check_only:
        print("\nâœ… ä¾èµ–æ£€æŸ¥å®Œæˆ")
        return 0
    
    # æ‰§è¡Œé‡åŒ–
    quantize_model(
        model_path=args.model_path,
        quant_path=args.quant_path,
        calib_data=args.calib_data,
        max_calib_seq_len=args.max_calib_seq_len,
        max_calib_samples=args.max_calib_samples,
        w_bit=args.w_bit,
        q_group_size=args.q_group_size,
    )
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
