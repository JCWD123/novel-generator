"""
AWQ é‡åŒ–è„šæœ¬ - å°†åŸå§‹æ¨¡å‹é‡åŒ–ä¸º AWQ æ ¼å¼

æ”¯æŒä¸¤ç§éƒ¨ç½²æ–¹å¼ï¼š
1. ä½¿ç”¨ transformers ç›´æ¥åŠ è½½ (Gradio å‰ç«¯)
2. ä½¿ç”¨ vLLM æœåŠ¡å™¨éƒ¨ç½² (Streamlit å‰ç«¯)

ç”¨æ³•ï¼š
    # åŸºæœ¬ç”¨æ³•
    python awq_quantization.py --model_path /path/to/model --quant_path /path/to/output
    
    # ä½¿ç”¨è‡ªå®šä¹‰æ ¡å‡†æ•°æ®é›†
    python awq_quantization.py --model_path /path/to/model --quant_path /path/to/output --calib_data wikitext
    
    # æŒ‡å®šé‡åŒ–ä½å®½
    python awq_quantization.py --model_path /path/to/model --quant_path /path/to/output --w_bit 4 --q_group_size 128

ä¾èµ–ï¼š
    pip install autoawq transformers torch

é‡åŒ–åä½¿ç”¨ Gradio å‰ç«¯éƒ¨ç½²ï¼š
    python gradio_app.py --model_path /path/to/output --auto_load
"""

import argparse
import os
import json
from pathlib import Path

try:
    from awq import AutoAWQForCausalLM
    from transformers import AutoTokenizer, AwqConfig
    HAS_AWQ = True
except ImportError:
    HAS_AWQ = False


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å®‰è£…"""
    if not HAS_AWQ:
        print("="*60)
        print("âŒ autoawq æœªå®‰è£…")
        print("="*60)
        print("\nå®‰è£…æ–¹æ³•:")
        print("  pip install autoawq")
        print("\næˆ–è€…ä½¿ç”¨ CUDA ç‰¹å®šç‰ˆæœ¬:")
        print("  pip install autoawq --extra-index-url https://download.pytorch.org/whl/cu121")
        print("="*60)
        return False
    return True


def validate_model_path(model_path: str) -> bool:
    """éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦æœ‰æ•ˆ"""
    path = Path(model_path)
    
    # æ£€æŸ¥æ˜¯å¦ä¸º HuggingFace æ¨¡å‹ ID æ ¼å¼
    if "/" in model_path and not path.exists():
        print(f"ğŸ“¦ æ£€æµ‹åˆ° HuggingFace æ¨¡å‹ ID: {model_path}")
        print("   å°†è‡ªåŠ¨ä» HuggingFace Hub ä¸‹è½½æ¨¡å‹")
        return True
    
    # æ£€æŸ¥æœ¬åœ°è·¯å¾„
    if not path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    config_file = path / "config.json"
    if not config_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ° config.json: {config_file}")
        return False
    
    print(f"âœ… æ¨¡å‹è·¯å¾„æœ‰æ•ˆ: {model_path}")
    return True


def get_model_info(model_path: str) -> dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    info = {"name": model_path, "size": "unknown"}
    
    config_path = Path(model_path) / "config.json"
    if config_path.exists():
        with open(config_path, "r") as f:
            config = json.load(f)
            info["hidden_size"] = config.get("hidden_size", "unknown")
            info["num_layers"] = config.get("num_hidden_layers", "unknown")
            info["vocab_size"] = config.get("vocab_size", "unknown")
            info["model_type"] = config.get("model_type", "unknown")
    
    return info


def quantize_model(
    model_path: str,
    quant_path: str,
    calib_data: str = "pileval",
    max_calib_seq_len: int = 1024,
    w_bit: int = 4,
    q_group_size: int = 128,
    zero_point: bool = True,
    version: str = "GEMM"
):
    """
    æ‰§è¡Œ AWQ é‡åŒ–
    
    Args:
        model_path: åŸå§‹æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID
        quant_path: é‡åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„
        calib_data: æ ¡å‡†æ•°æ®é›† (pileval, wikitext, c4, æˆ–è‡ªå®šä¹‰ HuggingFace æ•°æ®é›†)
        max_calib_seq_len: æ ¡å‡†æ—¶çš„æœ€å¤§åºåˆ—é•¿åº¦
        w_bit: é‡åŒ–ä½å®½ (é€šå¸¸ä¸º 4)
        q_group_size: é‡åŒ–åˆ†ç»„å¤§å° (64 æˆ– 128)
        zero_point: æ˜¯å¦ä½¿ç”¨é›¶ç‚¹é‡åŒ–
        version: é‡åŒ–ç‰ˆæœ¬ (GEMM æˆ– GEMV)
    """
    
    # é‡åŒ–é…ç½®
    quant_config = {
        "zero_point": zero_point,
        "q_group_size": q_group_size,
        "w_bit": w_bit,
        "version": version
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ”§ AWQ é‡åŒ–é…ç½®")
    print(f"{'='*60}")
    print(f"  ğŸ“¦ åŸå§‹æ¨¡å‹: {model_path}")
    print(f"  ğŸ’¾ è¾“å‡ºè·¯å¾„: {quant_path}")
    print(f"  ğŸ“Š æ ¡å‡†æ•°æ®: {calib_data}")
    print(f"  ğŸ“ æœ€å¤§åºåˆ—é•¿åº¦: {max_calib_seq_len}")
    print(f"  ğŸ”¢ é‡åŒ–ä½å®½: {w_bit} bit")
    print(f"  ğŸ“ åˆ†ç»„å¤§å°: {q_group_size}")
    print(f"  ğŸ¯ é›¶ç‚¹é‡åŒ–: {zero_point}")
    print(f"  âš™ï¸ ç‰ˆæœ¬: {version}")
    print(f"{'='*60}\n")
    
    # è·å–æ¨¡å‹ä¿¡æ¯
    model_info = get_model_info(model_path)
    if model_info.get("hidden_size") != "unknown":
        print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   ç±»å‹: {model_info.get('model_type', 'unknown')}")
        print(f"   éšè—å±‚ç»´åº¦: {model_info.get('hidden_size', 'unknown')}")
        print(f"   å±‚æ•°: {model_info.get('num_layers', 'unknown')}")
        print(f"   è¯è¡¨å¤§å°: {model_info.get('vocab_size', 'unknown')}")
        print()
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½åŸå§‹æ¨¡å‹...")
    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        safetensors=True
    )
    
    print("ğŸ“ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    # æ‰§è¡Œé‡åŒ–
    print(f"\nğŸ”§ å¼€å§‹é‡åŒ– (ä½¿ç”¨ {calib_data} æ•°æ®é›†)...")
    print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿåˆ°å‡ å°æ—¶ï¼Œå–å†³äºæ¨¡å‹å¤§å°...")
    
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        calib_data=calib_data,
        max_calib_seq_len=max_calib_seq_len
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(quant_path, exist_ok=True)
    
    # ä¿å­˜é‡åŒ–é…ç½®åˆ° config.json
    print("\nğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
    quantization_config = AwqConfig(
        bits=quant_config["w_bit"],
        group_size=quant_config["q_group_size"],
        zero_point=quant_config["zero_point"],
        version=quant_config["version"].lower(),
    ).to_dict()
    
    model.model.config.quantization_config = quantization_config
    
    # ä¿å­˜æ¨¡å‹å’Œ tokenizer
    model.save_quantized(quant_path)
    tokenizer.save_pretrained(quant_path)
    
    # è®¡ç®—å‹ç¼©ç‡
    original_size = sum(p.numel() * 2 for p in model.model.parameters()) / (1024**3)  # å‡è®¾ fp16
    quantized_size = sum(p.numel() * w_bit / 8 for p in model.model.parameters()) / (1024**3)
    compression_ratio = original_size / quantized_size if quantized_size > 0 else 0
    
    print(f"\n{'='*60}")
    print(f"âœ… é‡åŒ–å®Œæˆ!")
    print(f"{'='*60}")
    print(f"  ğŸ“ è¾“å‡ºè·¯å¾„: {quant_path}")
    print(f"  ğŸ“Š å‹ç¼©æ¯”: ~{compression_ratio:.1f}x")
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"\n  æ–¹å¼1: Gradio å‰ç«¯ (transformers ç›´æ¥åŠ è½½)")
    print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  python gradio_app.py --model_path {quant_path} --auto_load")
    print(f"\n  æ–¹å¼2: vLLM æœåŠ¡å™¨ (Streamlit å‰ç«¯)")
    print(f"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"  python start_vllm_server.py --model {quant_path} --quantization awq")
    print(f"  python -m streamlit run app.py")
    print(f"{'='*60}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="AWQ é‡åŒ–è„šæœ¬ - å°†æ¨¡å‹é‡åŒ–ä¸º 4-bit AWQ æ ¼å¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä½¿ç”¨é»˜è®¤é…ç½®é‡åŒ– DeepSeek-R1 7B
  python awq_quantization.py \\
      --model_path deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\
      --quant_path ./models/DeepSeek-R1-7B-AWQ

  # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
  python awq_quantization.py \\
      --model_path /path/to/model \\
      --quant_path /path/to/output \\
      --calib_data wikitext \\
      --w_bit 4 \\
      --q_group_size 128

æ ¡å‡†æ•°æ®é›†é€‰é¡¹:
  - pileval: é»˜è®¤ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯
  - wikitext: ç»´åŸºç™¾ç§‘æ–‡æœ¬
  - c4: Common Crawl æ•°æ®é›†
  - ä¹Ÿå¯ä»¥ä½¿ç”¨ HuggingFace ä¸Šçš„å…¶ä»–æ•°æ®é›†
        """
    )
    
    parser.add_argument(
        "--model_path",
        type=str,
        required=True,
        help="åŸå§‹æ¨¡å‹è·¯å¾„æˆ– HuggingFace æ¨¡å‹ ID"
    )
    parser.add_argument(
        "--quant_path",
        type=str,
        required=True,
        help="é‡åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„"
    )
    parser.add_argument(
        "--calib_data",
        type=str,
        default="pileval",
        help="æ ¡å‡†æ•°æ®é›† (é»˜è®¤: pileval)"
    )
    parser.add_argument(
        "--max_calib_seq_len",
        type=int,
        default=1024,
        help="æ ¡å‡†æ—¶çš„æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 1024)"
    )
    parser.add_argument(
        "--w_bit",
        type=int,
        default=4,
        choices=[2, 3, 4, 8],
        help="é‡åŒ–ä½å®½ (é»˜è®¤: 4)"
    )
    parser.add_argument(
        "--q_group_size",
        type=int,
        default=128,
        choices=[32, 64, 128, 256],
        help="é‡åŒ–åˆ†ç»„å¤§å° (é»˜è®¤: 128)"
    )
    parser.add_argument(
        "--no_zero_point",
        action="store_true",
        help="ç¦ç”¨é›¶ç‚¹é‡åŒ–"
    )
    parser.add_argument(
        "--version",
        type=str,
        default="GEMM",
        choices=["GEMM", "GEMV"],
        help="é‡åŒ–ç‰ˆæœ¬ (é»˜è®¤: GEMM)"
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        exit(1)
    
    # éªŒè¯æ¨¡å‹è·¯å¾„
    if not validate_model_path(args.model_path):
        exit(1)
    
    # æ‰§è¡Œé‡åŒ–
    try:
        quantize_model(
            model_path=args.model_path,
            quant_path=args.quant_path,
            calib_data=args.calib_data,
            max_calib_seq_len=args.max_calib_seq_len,
            w_bit=args.w_bit,
            q_group_size=args.q_group_size,
            zero_point=not args.no_zero_point,
            version=args.version
        )
    except Exception as e:
        print(f"\nâŒ é‡åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
