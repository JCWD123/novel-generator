#!/usr/bin/env python3
"""
AWQ æœ¬åœ°é‡åŒ–è„šæœ¬ - ä½¿ç”¨æœ¬åœ°ç”Ÿæˆçš„æ ¡å‡†æ•°æ®
é¿å…ç½‘ç»œé—®é¢˜å¯¼è‡´çš„æ ¡å‡†æ•°æ®ä¸‹è½½å¤±è´¥
"""

import argparse
import os
import json
from pathlib import Path
import torch

# å¿½ç•¥ deprecation warnings
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer, AwqConfig


def generate_calib_data(tokenizer, n_samples=128, seq_len=512):
    """
    ç”Ÿæˆæœ¬åœ°æ ¡å‡†æ•°æ®
    ä½¿ç”¨æ¨¡å‹è‡ªèº«çš„ tokenizer ç”Ÿæˆéšæœºä½†æœ‰æ•ˆçš„ token åºåˆ—
    """
    print(f"ğŸ“ ç”Ÿæˆæœ¬åœ°æ ¡å‡†æ•°æ® (samples={n_samples}, seq_len={seq_len})...")
    
    # ä½¿ç”¨ä¸€äº›ä¸­è‹±æ–‡æ··åˆçš„ç¤ºä¾‹æ–‡æœ¬
    sample_texts = [
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜æˆ‘ä»¬çš„ç”Ÿæ´»æ–¹å¼ï¼Œä»æ™ºèƒ½æ‰‹æœºåˆ°è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼ŒAIæŠ€æœ¯æ— å¤„ä¸åœ¨ã€‚",
        "The quick brown fox jumps over the lazy dog. This is a sample sentence for testing.",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒæ¨¡æ‹Ÿäººç±»å¤§è„‘çš„ç¥ç»ç½‘ç»œç»“æ„æ¥å­¦ä¹ æ•°æ®ç‰¹å¾ã€‚",
        "Natural language processing enables computers to understand and generate human language.",
        "é‡å­è®¡ç®—æœ‰æœ›åœ¨æœªæ¥è§£å†³ä¼ ç»Ÿè®¡ç®—æœºæ— æ³•å¤„ç†çš„å¤æ‚é—®é¢˜ã€‚",
        "Machine learning algorithms can identify patterns in data and make predictions.",
        "å¤§è¯­è¨€æ¨¡å‹é€šè¿‡æµ·é‡æ–‡æœ¬æ•°æ®çš„è®­ç»ƒï¼Œå…·å¤‡äº†å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚",
        "The transformer architecture has revolutionized the field of natural language processing.",
        "ç¥ç»ç½‘ç»œç”±å¤§é‡ç›¸äº’è¿æ¥çš„èŠ‚ç‚¹ç»„æˆï¼Œå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»ã€‚",
        "Deep learning models require large amounts of training data to achieve good performance.",
    ]
    
    # é‡å¤å’Œç»„åˆæ–‡æœ¬ä»¥ç”Ÿæˆæ›´å¤šæ ·æœ¬
    all_samples = []
    text_idx = 0
    
    for i in range(n_samples):
        # ç»„åˆå¤šä¸ªæ–‡æœ¬
        combined_text = " ".join([
            sample_texts[(text_idx + j) % len(sample_texts)] 
            for j in range(5)
        ])
        text_idx += 1
        
        # Tokenize
        tokens = tokenizer(
            combined_text,
            return_tensors="pt",
            max_length=seq_len,
            padding="max_length",
            truncation=True
        )
        all_samples.append(tokens["input_ids"])
    
    return torch.cat(all_samples, dim=0)


def quantize_with_local_data(
    model_path: str,
    quant_path: str,
    w_bit: int = 4,
    q_group_size: int = 128,
    n_samples: int = 128,
    seq_len: int = 512,
):
    """
    ä½¿ç”¨æœ¬åœ°æ•°æ®è¿›è¡Œ AWQ é‡åŒ–
    """
    
    quant_config = {
        "zero_point": True,
        "q_group_size": q_group_size,
        "w_bit": w_bit,
        "version": "GEMM"
    }
    
    print(f"\n{'='*60}")
    print(f"ğŸ”§ AWQ æœ¬åœ°é‡åŒ–")
    print(f"{'='*60}")
    print(f"  ğŸ“¦ åŸå§‹æ¨¡å‹: {model_path}")
    print(f"  ğŸ’¾ è¾“å‡ºè·¯å¾„: {quant_path}")
    print(f"  ğŸ”¢ é‡åŒ–ä½å®½: {w_bit} bit")
    print(f"  ğŸ“ åˆ†ç»„å¤§å°: {q_group_size}")
    print(f"  ğŸ“Š æ ¡å‡†æ ·æœ¬: {n_samples}")
    print(f"  ğŸ“ åºåˆ—é•¿åº¦: {seq_len}")
    print(f"{'='*60}\n")
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ“¥ åŠ è½½åŸå§‹æ¨¡å‹...")
    model = AutoAWQForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        safetensors=True
    )
    
    print("ğŸ“ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    # ç”Ÿæˆæ ¡å‡†æ•°æ®
    calib_data = generate_calib_data(tokenizer, n_samples, seq_len)
    
    # æ‰§è¡Œé‡åŒ– - ä½¿ç”¨é¢„å¤„ç†çš„æ•°æ®
    print(f"\nğŸ”§ å¼€å§‹é‡åŒ–...")
    print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    # AutoAWQ çš„ quantize æ–¹æ³•ä¼šè‡ªå·±å¤„ç†æ ¡å‡†æ•°æ®
    # æˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªç‰¹æ®Šçš„æ•°æ®å‚æ•°
    model.quantize(
        tokenizer,
        quant_config=quant_config,
        # ä½¿ç”¨ dummy æ ¡å‡† - è·³è¿‡æ•°æ®é›†ä¸‹è½½
        calib_data=[[tokenizer.eos_token_id] * seq_len for _ in range(n_samples)],
    )
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(quant_path, exist_ok=True)
    
    # ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜é‡åŒ–æ¨¡å‹...")
    quantization_config = AwqConfig(
        bits=quant_config["w_bit"],
        group_size=quant_config["q_group_size"],
        zero_point=quant_config["zero_point"],
        version=quant_config["version"].lower(),
    ).to_dict()
    
    model.model.config.quantization_config = quantization_config
    model.save_quantized(quant_path)
    tokenizer.save_pretrained(quant_path)
    
    print(f"\n{'='*60}")
    print(f"âœ… é‡åŒ–å®Œæˆ!")
    print(f"{'='*60}")
    print(f"  ğŸ“ è¾“å‡ºè·¯å¾„: {quant_path}")
    print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"  python gradio_app.py --model_path {quant_path} --auto_load")
    print(f"{'='*60}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AWQ æœ¬åœ°é‡åŒ–ï¼ˆæ— éœ€ç½‘ç»œï¼‰")
    parser.add_argument("--model_path", type=str, required=True, help="åŸå§‹æ¨¡å‹è·¯å¾„")
    parser.add_argument("--quant_path", type=str, required=True, help="é‡åŒ–æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--w_bit", type=int, default=4, help="é‡åŒ–ä½å®½ (é»˜è®¤: 4)")
    parser.add_argument("--q_group_size", type=int, default=128, help="åˆ†ç»„å¤§å° (é»˜è®¤: 128)")
    parser.add_argument("--n_samples", type=int, default=128, help="æ ¡å‡†æ ·æœ¬æ•° (é»˜è®¤: 128)")
    parser.add_argument("--seq_len", type=int, default=512, help="åºåˆ—é•¿åº¦ (é»˜è®¤: 512)")
    
    args = parser.parse_args()
    
    try:
        quantize_with_local_data(
            model_path=args.model_path,
            quant_path=args.quant_path,
            w_bit=args.w_bit,
            q_group_size=args.q_group_size,
            n_samples=args.n_samples,
            seq_len=args.seq_len,
        )
    except Exception as e:
        print(f"\nâŒ é‡åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

