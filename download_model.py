#!/usr/bin/env python3
"""
æ¨¡å‹ä¸‹è½½è„šæœ¬ - æ¨¡ä»¿ eedi-mining-misconceptions é¡¹ç›®

æ”¯æŒä¸¤ç§ä¸‹è½½æ–¹å¼ï¼š
1. ä» HuggingFace ä¸‹è½½ï¼ˆä½¿ç”¨ hf_transfer åŠ é€Ÿï¼‰
2. ä» Kaggle ä¸‹è½½ï¼ˆä½¿ç”¨ kagglehubï¼‰

ç”¨æ³•ï¼š
    # ä» HuggingFace ä¸‹è½½ï¼ˆæ¨èï¼‰
    HF_HUB_ENABLE_HF_TRANSFER=1 python download_model.py --model 7b
    
    # ä½¿ç”¨é•œåƒåŠ é€Ÿ
    HF_HUB_ENABLE_HF_TRANSFER=1 python download_model.py --model 7b --mirror
    
    # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
    python download_model.py --all
"""

import os
import subprocess
import sys

# DeepSeek-R1 è’¸é¦ç‰ˆæ¨¡å‹åˆ—è¡¨
MODELS = {
    "1.5b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
}


def download_from_hf(model_id: str, use_mirror: bool = False) -> None:
    """
    ä» HuggingFace ä¸‹è½½æ¨¡å‹
    
    å€Ÿé‰´ Train-parts-eedi é¡¹ç›®çš„ä¸‹è½½æ–¹å¼ï¼š
    HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download <model>
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ Downloading: {model_id}")
    print(f"{'='*60}")
    
    env = os.environ.copy()
    
    # å¯ç”¨ hf_transfer åŠ é€Ÿ
    env["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
    
    # ä½¿ç”¨é•œåƒ
    if use_mirror:
        env["HF_ENDPOINT"] = "https://hf-mirror.com"
        print("ğŸŒ Using mirror: hf-mirror.com")
    
    # ä½¿ç”¨ huggingface-cli download
    cmd = ["huggingface-cli", "download", model_id]
    
    try:
        subprocess.run(cmd, env=env, check=True)
        print(f"\nâœ… Downloaded: {model_id}")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ Failed to download: {e}")
        sys.exit(1)
    except FileNotFoundError:
        print("âŒ huggingface-cli not found. Installing...")
        subprocess.run([sys.executable, "-m", "pip", "install", "huggingface_hub[hf_transfer]", "-q"])
        subprocess.run(cmd, env=env, check=True)


def download_from_kaggle(handle: str) -> None:
    """
    ä» Kaggle ä¸‹è½½æ¨¡å‹ - æ¨¡ä»¿ eedi-mining-misconceptions é¡¹ç›®
    """
    try:
        import kagglehub
        
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ Downloading from Kaggle: {handle}")
        print(f"{'='*60}")
        
        local_dir = kagglehub.model_download(handle)
        print(f"âœ… Downloaded to: {local_dir}")
        
    except Exception as e:
        print(f"âŒ Failed to download: {e}")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Download DeepSeek-R1 models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Models:
  1.5b  -> deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  (~3GB)
  7b    -> deepseek-ai/DeepSeek-R1-Distill-Qwen-7B    (~14GB)
  14b   -> deepseek-ai/DeepSeek-R1-Distill-Qwen-14B   (~28GB)
  32b   -> deepseek-ai/DeepSeek-R1-Distill-Qwen-32B   (~64GB)
  70b   -> deepseek-ai/DeepSeek-R1-Distill-Llama-70B  (~140GB)

Examples:
  # Download with hf_transfer acceleration
  HF_HUB_ENABLE_HF_TRANSFER=1 python download_model.py --model 7b
  
  # Use mirror (for China)
  HF_HUB_ENABLE_HF_TRANSFER=1 python download_model.py --model 7b --mirror
        """
    )
    
    parser.add_argument("--model", "-m", type=str, choices=list(MODELS.keys()),
                        help="Model to download (1.5b/7b/14b/32b/70b)")
    parser.add_argument("--model-id", type=str, help="Full HuggingFace model ID")
    parser.add_argument("--mirror", action="store_true", help="Use hf-mirror.com")
    parser.add_argument("--all", action="store_true", help="Download all models")
    parser.add_argument("--kaggle", type=str, help="Kaggle model handle")
    
    args = parser.parse_args()
    
    # ä» Kaggle ä¸‹è½½
    if args.kaggle:
        download_from_kaggle(args.kaggle)
        return
    
    # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
    if args.all:
        for key, model_id in MODELS.items():
            download_from_hf(model_id, args.mirror)
        return
    
    # ä¸‹è½½æŒ‡å®šæ¨¡å‹
    if args.model:
        model_id = MODELS[args.model]
    elif args.model_id:
        model_id = args.model_id
    else:
        parser.print_help()
        print("\nâŒ Please specify --model or --model-id")
        sys.exit(1)
    
    download_from_hf(model_id, args.mirror)
    
    print("\n" + "="*60)
    print("âœ… Download complete!")
    print("Next: python start_vllm_server.py --model", model_id)
    print("="*60)


if __name__ == "__main__":
    main()
