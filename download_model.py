#!/usr/bin/env python3
"""
æ¨¡å‹ä¸‹è½½è„šæœ¬ - å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™åŠ é€Ÿ

ç”¨æ³•ï¼š
    python download_model.py --model 7b
    python download_model.py --model 1.5b
    python download_model.py --all
"""

import os
import subprocess
import sys

# å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™ - ç§»é™¤æ‰€æœ‰ç›´è¿é€»è¾‘
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

# DeepSeek-R1 è’¸é¦ç‰ˆæ¨¡å‹åˆ—è¡¨
MODELS = {
    "1.5b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
}


def check_local_model(model_id: str) -> bool:
    """æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½åˆ°æœ¬åœ°"""
    try:
        from huggingface_hub import scan_cache_dir
        
        cache_info = scan_cache_dir()
        
        for repo in cache_info.repos:
            if repo.repo_id == model_id:
                if repo.revisions:
                    for revision in repo.revisions:
                        snapshot_path = revision.snapshot_path
                        if snapshot_path.exists():
                            model_files = list(snapshot_path.glob("*.safetensors")) + list(snapshot_path.glob("*.bin"))
                            if model_files:
                                print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {snapshot_path}")
                                return True
        return False
        
    except Exception:
        return False


def download_from_hf(model_id: str) -> None:
    """
    ä» HuggingFace é•œåƒç«™ä¸‹è½½æ¨¡å‹
    """
    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
    if check_local_model(model_id):
        print(f"â­ï¸ è·³è¿‡ä¸‹è½½ï¼Œæ¨¡å‹å·²å­˜åœ¨")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {model_id}")
    print(f"ğŸŒ é•œåƒç«™: {os.environ.get('HF_ENDPOINT')}")
    print(f"{'='*60}")
    
    # ä½¿ç”¨ huggingface-cli download
    cmd = ["huggingface-cli", "download", model_id]
    
    try:
        subprocess.run(cmd, check=True)
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {model_id}")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)
    except FileNotFoundError:
        print("âŒ huggingface-cli æœªæ‰¾åˆ°ï¼Œæ­£åœ¨å®‰è£…...")
        subprocess.run([sys.executable, "-m", "pip", "install", "huggingface_hub[hf_transfer]", "-q"])
        subprocess.run(cmd, check=True)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½ DeepSeek-R1 æ¨¡å‹ï¼ˆå¼ºåˆ¶ä½¿ç”¨é•œåƒç«™ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ¨¡å‹åˆ—è¡¨:
  1.5b  -> deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B  (~3GB)
  7b    -> deepseek-ai/DeepSeek-R1-Distill-Qwen-7B    (~14GB)
  14b   -> deepseek-ai/DeepSeek-R1-Distill-Qwen-14B   (~28GB)
  32b   -> deepseek-ai/DeepSeek-R1-Distill-Qwen-32B   (~64GB)
  70b   -> deepseek-ai/DeepSeek-R1-Distill-Llama-70B  (~140GB)

ç¤ºä¾‹:
  python download_model.py --model 7b
  python download_model.py --model 1.5b
  python download_model.py --all
        """
    )
    
    parser.add_argument("--model", "-m", type=str, choices=list(MODELS.keys()),
                        help="æ¨¡å‹åç§° (1.5b/7b/14b/32b/70b)")
    parser.add_argument("--model-id", type=str, help="å®Œæ•´ HuggingFace æ¨¡å‹ ID")
    parser.add_argument("--all", action="store_true", help="ä¸‹è½½æ‰€æœ‰æ¨¡å‹")
    
    args = parser.parse_args()
    
    print(f"ğŸŒ ä½¿ç”¨é•œåƒç«™: {os.environ.get('HF_ENDPOINT')}")
    print(f"âš¡ hf_transfer åŠ é€Ÿ: å·²å¯ç”¨")
    
    # ä¸‹è½½æ‰€æœ‰æ¨¡å‹
    if args.all:
        for key, model_id in MODELS.items():
            download_from_hf(model_id)
        return
    
    # ä¸‹è½½æŒ‡å®šæ¨¡å‹
    if args.model:
        model_id = MODELS[args.model]
    elif args.model_id:
        model_id = args.model_id
    else:
        parser.print_help()
        print("\nâŒ è¯·æŒ‡å®š --model æˆ– --model-id")
        sys.exit(1)
    
    download_from_hf(model_id)
    
    print("\n" + "="*60)
    print("âœ… ä¸‹è½½å®Œæˆ!")
    print(f"ä¸‹ä¸€æ­¥: python start_vllm_server.py --model {args.model or model_id}")
    print("="*60)


if __name__ == "__main__":
    main()
