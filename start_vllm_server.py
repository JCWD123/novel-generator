#!/usr/bin/env python3
"""
vLLM Ê®°ÂûãÊúçÂä°ÂêØÂä®ËÑöÊú¨

ÊîØÊåÅÊú¨Âú∞Ê®°ÂûãÂíå HuggingFace Âú®Á∫øÊ®°ÂûãÔºåÂ∏¶Êúâ‰∫§‰∫íÂºèÈÖçÁΩÆÂíåËá™Âä®Âº†ÈáèÂπ∂Ë°å„ÄÇ

Êñ∞Â¢ûÂäüËÉΩÔºö
- ‰∫§‰∫íÂºèÊ®°ÂºèÔºöËá™Âä®Ê£ÄÊµã GPU ÊòæÂ≠òÂπ∂ÁªôÂá∫ÈÉ®ÁΩ≤Âª∫ËÆÆ
- Ëá™Âä®Âº†ÈáèÂπ∂Ë°åÔºöÂΩìÂçïÂç°ÊòæÂ≠ò‰∏çË∂≥Êó∂ÔºåËá™Âä®ÊèêÁ§∫‰ΩøÁî®Â§öÂç°
- Êô∫ËÉΩÈÖçÁΩÆÔºöÊ†πÊçÆÊòæÂ≠òËá™Âä®ÈÄâÊã©ÊúÄ‰ºòÈÖçÁΩÆ

Áî®Ê≥ïÔºö
    # ‰∫§‰∫íÂºèÊ®°ÂºèÔºàÊé®ËçêÔºåËá™Âä®Ê£ÄÊµãÊòæÂ≠òÔºâ
    python start_vllm_server.py --interactive
    
    # Ëá™Âä®Âº†ÈáèÂπ∂Ë°åÔºàÊòæÂ≠ò‰∏çË∂≥Êó∂Ëá™Âä®ÊèêÁ§∫Ôºâ
    python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B --auto-tp
    
    # ÂèåÂç°Âº†ÈáèÂπ∂Ë°å + BNB ÈáèÂåñ
    python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \\
        --quantization bnb --tensor-parallel 2
    
    # ‰ΩøÁî®Êú¨Âú∞Ê®°ÂûãË∑ØÂæÑ
    python start_vllm_server.py --model /home/user/models/my-model --tensor-parallel 2
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path
from typing import Optional, List, Dict

# ==================== GPU Ê£ÄÊµãÂäüËÉΩ ====================

def get_gpu_info() -> List[Dict]:
    """Ëé∑ÂèñÊâÄÊúâ GPU ÁöÑËØ¶ÁªÜ‰ø°ÊÅØ"""
    gpu_list = []
    
    try:
        import torch
        if not torch.cuda.is_available():
            return gpu_list
        
        num_gpus = torch.cuda.device_count()
        
        for i in range(num_gpus):
            props = torch.cuda.get_device_properties(i)
            total_mem = props.total_memory / (1024**3)
            
            torch.cuda.set_device(i)
            free_mem, _ = torch.cuda.mem_get_info(i)
            free_mem = free_mem / (1024**3)
            used_mem = total_mem - free_mem
            
            gpu_list.append({
                "index": i,
                "name": props.name,
                "total_memory": round(total_mem, 1),
                "free_memory": round(free_mem, 1),
                "used_memory": round(used_mem, 1),
            })
    except ImportError:
        print("‚ö†Ô∏è PyTorch Êú™ÂÆâË£ÖÔºåÊó†Ê≥ïÊ£ÄÊµã GPU")
    except Exception as e:
        print(f"‚ö†Ô∏è GPU Ê£ÄÊµãÂ§±Ë¥•: {e}")
    
    return gpu_list


def print_gpu_info(gpu_list: List[Dict]) -> float:
    """ÊâìÂç∞ GPU ‰ø°ÊÅØÂπ∂ËøîÂõûÊÄªÂèØÁî®ÊòæÂ≠ò"""
    if not gpu_list:
        print("\n‚ùå Êú™Ê£ÄÊµãÂà∞ÂèØÁî®ÁöÑ NVIDIA GPU")
        return 0
    
    print(f"\n{'='*70}")
    print(f"üñ•Ô∏è  GPU ‰ø°ÊÅØÊ£ÄÊµã")
    print(f"{'='*70}")
    
    total_available = 0
    for gpu in gpu_list:
        print(f"\n  GPU {gpu['index']}: {gpu['name']}")
        print(f"    ‚îú‚îÄ ÊÄªÊòæÂ≠ò: {gpu['total_memory']:.1f} GB")
        print(f"    ‚îú‚îÄ Â∑≤‰ΩøÁî®: {gpu['used_memory']:.1f} GB")
        print(f"    ‚îî‚îÄ ÂèØÁî®:   {gpu['free_memory']:.1f} GB")
        total_available += gpu['free_memory']
    
    print(f"\n  üìä ÊÄªÂèØÁî®ÊòæÂ≠ò: {total_available:.1f} GB ({len(gpu_list)} Âº†Âç°)")
    print(f"{'='*70}")
    
    return total_available


def estimate_model_memory(model_name: str, quantization: str = None, max_model_len: int = 8192) -> float:
    """‰º∞ÁÆóÊ®°ÂûãÊâÄÈúÄÊòæÂ≠ò (GB)"""
    # Âü∫Á°ÄÊòæÂ≠ò‰º∞ÁÆó
    model_memory = {
        "1.5b": 3.0,
        "7b": 14.0,
        "14b": 28.0,
        "32b": 64.0,
        "70b": 140.0,
    }
    
    # Â∞ùËØï‰ªéÊ®°ÂûãÂêçÊé®Êñ≠Â§ßÂ∞è
    base_memory = 14.0  # ÈªòËÆ§ 7B
    model_lower = model_name.lower()
    for size, mem in model_memory.items():
        if size in model_lower:
            base_memory = mem
            break
    
    # ÈáèÂåñÂêéÊòæÂ≠ò‰º∞ÁÆó
    if quantization in ["bnb", "bitsandbytes"]:
        base_memory *= 0.30
    elif quantization == "awq":
        base_memory *= 0.25
    elif quantization == "gptq":
        base_memory *= 0.25
    elif quantization == "fp8":
        base_memory *= 0.50
    
    # KV cache ‰º∞ÁÆó
    kv_cache = (max_model_len / 8192) * 2.0
    
    # È¢ùÂ§ñÂºÄÈîÄ
    overhead = 2.0
    
    return base_memory + kv_cache + overhead


def suggest_deployment(
    gpu_list: List[Dict],
    model_name: str,
    quantization: str = None,
    max_model_len: int = 8192
) -> Dict:
    """Ê†πÊçÆ GPU ÈÖçÁΩÆÂª∫ËÆÆÊúÄ‰ºòÈÉ®ÁΩ≤ÊñπÊ°à"""
    if not gpu_list:
        return {"error": "Ê≤°ÊúâÂèØÁî®ÁöÑ GPU", "can_deploy": False}
    
    estimated = estimate_model_memory(model_name, quantization, max_model_len)
    single_gpu = gpu_list[0]['free_memory']
    total_mem = sum(g['free_memory'] for g in gpu_list)
    num_gpus = len(gpu_list)
    
    result = {
        "estimated_memory": estimated,
        "single_gpu_memory": single_gpu,
        "total_memory": total_mem,
        "num_gpus": num_gpus,
        "tensor_parallel": 1,
        "can_deploy": False,
        "message": "",
        "suggestions": [],
    }
    
    # ÂçïÂç°ÂèØ‰ª•ÈÉ®ÁΩ≤
    if single_gpu >= estimated * 1.1:
        result["can_deploy"] = True
        result["tensor_parallel"] = 1
        result["message"] = f"‚úÖ ÂçïÂç°Âç≥ÂèØÈÉ®ÁΩ≤ÔºàÈúÄË¶Å {estimated:.1f}GBÔºåÂèØÁî® {single_gpu:.1f}GBÔºâ"
        return result
    
    # ÈúÄË¶ÅÂ§öÂç°
    if num_gpus > 1 and total_mem >= estimated * 1.1:
        needed_gpus = 2
        for i in range(2, num_gpus + 1):
            if (total_mem / num_gpus) * i >= estimated:
                needed_gpus = i
                break
        
        result["can_deploy"] = True
        result["tensor_parallel"] = needed_gpus
        result["message"] = f"‚ö° Âª∫ËÆÆ‰ΩøÁî® {needed_gpus} Âº†Âç°Âº†ÈáèÂπ∂Ë°åÔºàÊØèÂç°Á∫¶ {estimated/needed_gpus:.1f}GBÔºâ"
        return result
    
    # ÊòæÂ≠ò‰∏çË∂≥ÔºåÁªôÂá∫Âª∫ËÆÆ
    result["message"] = f"‚ùå ÊòæÂ≠ò‰∏çË∂≥ÔºàÈúÄË¶Å {estimated:.1f}GBÔºåÂèØÁî® {total_mem:.1f}GBÔºâ"
    
    if quantization is None:
        quantized_mem = estimate_model_memory(model_name, "bnb", max_model_len)
        if single_gpu >= quantized_mem * 1.1:
            result["suggestions"].append(f"ÂêØÁî®ÈáèÂåñ (--quantization bnb)ÔºåÈúÄË¶ÅÁ∫¶ {quantized_mem:.1f}GB")
        elif num_gpus > 1 and total_mem >= quantized_mem * 1.1:
            result["suggestions"].append(f"ÂêØÁî®ÈáèÂåñ + Â§öÂç° (--quantization bnb --tensor-parallel {num_gpus})")
    
    if max_model_len > 4096:
        result["suggestions"].append("ÂáèÂ∞ëÂ∫èÂàóÈïøÂ∫¶ (--max-model-len 4096)")
    
    result["suggestions"].append("‰ΩøÁî®Êõ¥Â∞èÁöÑÊ®°Âûã")
    
    return result


def interactive_mode():
    """‰∫§‰∫íÂºèÊ®°Âºè"""
    print("\n" + "="*70)
    print("üöÄ vLLM ÊúçÂä°ÈÉ®ÁΩ≤ÂêëÂØº")
    print("="*70)
    
    gpu_list = get_gpu_info()
    total_mem = print_gpu_info(gpu_list)
    
    if not gpu_list:
        print("\n‚ùå Êó†Ê≥ïÁªßÁª≠ÔºåËØ∑Á°Æ‰øùÊúâÂèØÁî®ÁöÑ NVIDIA GPU")
        return None
    
    # ÈÄâÊã©Ê®°Âûã
    print("\n" + "-"*70)
    print("üì¶ ÈÄâÊã©Ê®°Âûã:")
    print("-"*70)
    
    models = [
        ("1", "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "1.5B ÂèÇÊï∞ÔºàÊµãËØïÁî®Ôºâ"),
        ("2", "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B", "7B ÂèÇÊï∞ÔºàÊé®ËçêÔºâ"),
        ("3", "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B", "14B ÂèÇÊï∞"),
        ("4", "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B", "32B ÂèÇÊï∞"),
        ("5", "deepseek-ai/DeepSeek-R1-Distill-Llama-70B", "70B ÂèÇÊï∞ÔºàÊúÄÈ´òË¥®ÈáèÔºâ"),
        ("6", "custom", "Ëá™ÂÆö‰πâÊ®°ÂûãË∑ØÂæÑ"),
    ]
    
    for idx, model, desc in models:
        print(f"  [{idx}] {desc}")
        print(f"      {model}")
    
    choice = input("\nËØ∑ÈÄâÊã© [1-6ÔºåÈªòËÆ§ 2]: ").strip() or "2"
    
    if choice == "6":
        model_name = input("ËØ∑ËæìÂÖ•Ê®°ÂûãË∑ØÂæÑ: ").strip()
    elif choice in ["1", "2", "3", "4", "5"]:
        model_name = models[int(choice) - 1][1]
    else:
        model_name = models[1][1]  # ÈªòËÆ§ 7B
    
    # ÈÄâÊã©ÈáèÂåñ
    print("\n" + "-"*70)
    print("üîß ÈÄâÊã©ÈáèÂåñÊñπÂºè:")
    print("-"*70)
    print("  [1] Êó†ÈáèÂåñÔºàÂÖ®Á≤æÂ∫¶ÔºåÈúÄË¶ÅÊõ¥Â§öÊòæÂ≠òÔºâ")
    print("  [2] BitsAndBytes 4-bitÔºàÊé®ËçêÔºåÊòæÂ≠òÈôç‰Ωé 70%Ôºâ")
    print("  [3] AWQ 4-bitÔºàÈúÄË¶ÅÈ¢ÑÈáèÂåñÊ®°ÂûãÔºâ")
    
    quant_choice = input("\nËØ∑ÈÄâÊã© [1-3ÔºåÈªòËÆ§ 2]: ").strip() or "2"
    quantization = {
        "1": None,
        "2": "bnb",
        "3": "awq"
    }.get(quant_choice, "bnb")
    
    # ÈÄâÊã©Â∫èÂàóÈïøÂ∫¶
    print("\n" + "-"*70)
    print("üìè ÈÄâÊã©ÊúÄÂ§ßÂ∫èÂàóÈïøÂ∫¶:")
    print("-"*70)
    print("  [1] 2048ÔºàËäÇÁúÅÊòæÂ≠òÔºâ")
    print("  [2] 4096ÔºàÊé®ËçêÔºâ")
    print("  [3] 8192ÔºàÊ†áÂáÜÔºâ")
    print("  [4] 16384ÔºàÈïøÊñáÊú¨Ôºâ")
    
    len_choice = input("\nËØ∑ÈÄâÊã© [1-4ÔºåÈªòËÆ§ 2]: ").strip() or "2"
    max_model_len = {
        "1": 2048,
        "2": 4096,
        "3": 8192,
        "4": 16384
    }.get(len_choice, 4096)
    
    # ÂàÜÊûêÈÖçÁΩÆ
    print("\n" + "-"*70)
    print("üìä ÈÖçÁΩÆÂàÜÊûê:")
    print("-"*70)
    
    suggestion = suggest_deployment(gpu_list, model_name, quantization, max_model_len)
    
    print(f"\n  Ê®°Âûã: {model_name}")
    print(f"  ÈáèÂåñ: {quantization or 'Êó†'}")
    print(f"  Â∫èÂàóÈïøÂ∫¶: {max_model_len}")
    print(f"  È¢Ñ‰º∞ÊòæÂ≠ò: {suggestion['estimated_memory']:.1f} GB")
    print(f"\n  {suggestion['message']}")
    
    tensor_parallel = suggestion.get('tensor_parallel', 1)
    
    # ÊòæÂ≠ò‰∏çË∂≥Â§ÑÁêÜ
    if not suggestion['can_deploy'] and suggestion.get('suggestions'):
        print("\n  üí° Âª∫ËÆÆ:")
        for i, sug in enumerate(suggestion['suggestions'], 1):
            print(f"    [{i}] {sug}")
        
        fix = input("\nÈÄâÊã©Ëß£ÂÜ≥ÊñπÊ°àÁºñÂè∑ (ÊàñÁõ¥Êé•ÂõûËΩ¶ÁªßÁª≠): ").strip()
        
        if fix == "1" and "ÈáèÂåñ" in suggestion['suggestions'][0]:
            quantization = "bnb"
            if "Â§öÂç°" in suggestion['suggestions'][0]:
                tensor_parallel = len(gpu_list)
            suggestion = suggest_deployment(gpu_list, model_name, quantization, max_model_len)
        elif fix == "2" and len(suggestion['suggestions']) > 1:
            max_model_len = 4096
            suggestion = suggest_deployment(gpu_list, model_name, quantization, max_model_len)
    
    # Á°ÆËÆ§
    print("\n" + "-"*70)
    print("üöÄ ÊúÄÁªàÈÖçÁΩÆ:")
    print("-"*70)
    print(f"  Ê®°Âûã: {model_name}")
    print(f"  ÈáèÂåñ: {quantization or 'Êó†'}")
    print(f"  Â∫èÂàóÈïøÂ∫¶: {max_model_len}")
    print(f"  Âº†ÈáèÂπ∂Ë°å: {tensor_parallel} GPU(s)")
    
    confirm = input("\nÁ°ÆËÆ§ÂêØÂä®Ôºü[Y/n]: ").strip().lower()
    
    if confirm != "n":
        return {
            "model": model_name,
            "quantization": quantization,
            "max_model_len": max_model_len,
            "tensor_parallel": tensor_parallel,
        }
    
    return None


# ==================== Ê®°ÂûãÈÖçÁΩÆ ====================

HF_MODELS = {
    "1.5b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
}

PRESETS = {
    "12gb": {
        "description": "12GB ÊòæÂç°ÈÖçÁΩÆ",
        "quantization": "bnb",
        "max_model_len": 4096,
        "gpu_memory": 0.92,
        "enforce_eager": True,
    },
    "24gb": {
        "description": "24GB ÊòæÂç°ÈÖçÁΩÆ",
        "quantization": "bnb",
        "max_model_len": 8192,
        "gpu_memory": 0.90,
        "enforce_eager": False,
    },
    "48gb": {
        "description": "48GB ÊòæÂç°ÈÖçÁΩÆ",
        "quantization": None,
        "max_model_len": 8192,
        "gpu_memory": 0.85,
        "enforce_eager": False,
    },
    "dual-48gb": {
        "description": "ÂèåÂç° 48GB ÈÖçÁΩÆÔºàÈÄÇÂêà 70BÔºâ",
        "quantization": "bnb",
        "max_model_len": 8192,
        "gpu_memory": 0.90,
        "tensor_parallel": 2,
        "enforce_eager": True,
    },
}

DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8000
DEFAULT_GPU_MEMORY = 0.90


def start_vllm_server(
    model: str,
    host: str = DEFAULT_HOST,
    port: int = DEFAULT_PORT,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = DEFAULT_GPU_MEMORY,
    max_model_len: int = None,
    quantization: str = None,
    enforce_eager: bool = False,
    dtype: str = None,
    served_model_name: str = "deepseek-r1",
):
    """ÂêØÂä® vLLM ÊúçÂä°Âô®"""
    
    # ÊûÑÂª∫ÂëΩ‰ª§
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", model,
        "--host", host,
        "--port", str(port),
        "--tensor-parallel-size", str(tensor_parallel_size),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--served-model-name", served_model_name,
        "--trust-remote-code",
    ]
    
    if max_model_len:
        cmd.extend(["--max-model-len", str(max_model_len)])
    
    if quantization:
        if quantization == "bnb":
            quantization = "bitsandbytes"
        cmd.extend(["--quantization", quantization])
    
    if enforce_eager:
        cmd.append("--enforce-eager")
    
    if dtype:
        cmd.extend(["--dtype", dtype])
    
    # ÊâìÂç∞ÈÖçÁΩÆ
    print("\n" + "="*70)
    print("üöÄ ÂêØÂä® vLLM ÊúçÂä°Âô®")
    print("="*70)
    print(f"üì¶ Ê®°Âûã: {model}")
    print(f"üîó Âú∞ÂùÄ: http://{host}:{port}")
    print(f"üéØ API ÂêçÁß∞: {served_model_name}")
    print(f"üíæ ÊòæÂ≠òÂà©Áî®Áéá: {gpu_memory_utilization:.0%}")
    print(f"üñ•Ô∏è GPU Êï∞Èáè: {tensor_parallel_size}")
    if max_model_len:
        print(f"üìè ÊúÄÂ§ßÈïøÂ∫¶: {max_model_len}")
    if quantization:
        print(f"üîß ÈáèÂåñ: {quantization}")
    print("="*70)
    
    print(f"\nüìù ÂëΩ‰ª§: {' '.join(cmd)}\n")
    
    # ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè
    env = os.environ.copy()
    env["TOKENIZERS_PARALLELISM"] = "false"
    env["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
    
    try:
        subprocess.run(cmd, env=env, check=True)
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è ÊúçÂä°Âô®Â∑≤ÂÅúÊ≠¢")
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå ÂêØÂä®Â§±Ë¥•: {e}")
        sys.exit(1)
    except FileNotFoundError:
        print("\n‚ùå Êâæ‰∏çÂà∞ vllmÔºåËØ∑ÂÆâË£Ö: pip install vllm>=0.6.0")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="vLLM ÊúçÂä°ÂêØÂä®ËÑöÊú¨ÔºàÊîØÊåÅ‰∫§‰∫íÂºèÈÖçÁΩÆÂíåËá™Âä®Âº†ÈáèÂπ∂Ë°åÔºâ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìå ‰ΩøÁî®Á§∫‰æãÔºö

  üîπ ‰∫§‰∫íÂºèÊ®°ÂºèÔºàÊé®ËçêÔºâÔºö
     python start_vllm_server.py --interactive

  üîπ Ëá™Âä®Âº†ÈáèÂπ∂Ë°åÔºö
     python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B --auto-tp

  üîπ ÂèåÂç° + BNB ÈáèÂåñÔºö
     python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Llama-70B \\
         --quantization bnb --tensor-parallel 2

  üîπ ‰ΩøÁî®Êú¨Âú∞Ê®°ÂûãÔºö
     python start_vllm_server.py --model /home/user/models/my-model --tensor-parallel 2

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
        """
    )
    
    parser.add_argument("--interactive", "-i", action="store_true", help="‰∫§‰∫íÂºèÊ®°Âºè")
    parser.add_argument("--auto-tp", action="store_true", help="Ëá™Âä®Âº†ÈáèÂπ∂Ë°å")
    parser.add_argument("--model", "-m", type=str, default="deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
    parser.add_argument("--host", type=str, default=DEFAULT_HOST)
    parser.add_argument("--port", "-p", type=int, default=DEFAULT_PORT)
    parser.add_argument("--tensor-parallel", "--tp", type=int, default=1)
    parser.add_argument("--gpu-memory", type=float, default=DEFAULT_GPU_MEMORY)
    parser.add_argument("--max-model-len", type=int, default=None)
    parser.add_argument("--quantization", "-q", type=str, choices=["bnb", "bitsandbytes", "awq", "gptq", "fp8"])
    parser.add_argument("--enforce-eager", action="store_true")
    parser.add_argument("--dtype", type=str, choices=["bfloat16", "float16", "auto"])
    parser.add_argument("--preset", type=str, choices=list(PRESETS.keys()))
    parser.add_argument("--gpu-info", action="store_true", help="ÊòæÁ§∫ GPU ‰ø°ÊÅØ")
    
    args = parser.parse_args()
    
    # GPU ‰ø°ÊÅØ
    if args.gpu_info:
        gpu_list = get_gpu_info()
        print_gpu_info(gpu_list)
        return 0
    
    # ‰∫§‰∫íÂºèÊ®°Âºè
    if args.interactive:
        config = interactive_mode()
        if config:
            start_vllm_server(
                model=config["model"],
                host=args.host,
                port=args.port,
                tensor_parallel_size=config.get("tensor_parallel", 1),
                gpu_memory_utilization=args.gpu_memory,
                max_model_len=config.get("max_model_len"),
                quantization=config.get("quantization"),
                enforce_eager=args.enforce_eager,
            )
        return 0
    
    # Ëá™Âä®Âº†ÈáèÂπ∂Ë°å
    tensor_parallel = args.tensor_parallel
    if args.auto_tp:
        gpu_list = get_gpu_info()
        if gpu_list:
            print_gpu_info(gpu_list)
            suggestion = suggest_deployment(
                gpu_list, args.model, args.quantization, args.max_model_len or 8192
            )
            
            if suggestion['tensor_parallel'] > 1:
                print(f"\n‚ö° Âª∫ËÆÆ‰ΩøÁî®Âº†ÈáèÂπ∂Ë°å: {suggestion['tensor_parallel']} GPUs")
                confirm = input(f"‰ΩøÁî® {suggestion['tensor_parallel']} Âº†Âç°Ôºü[Y/n]: ").strip().lower()
                if confirm != "n":
                    tensor_parallel = suggestion['tensor_parallel']
    
    # Â∫îÁî®È¢ÑËÆæ
    quantization = args.quantization
    max_model_len = args.max_model_len
    enforce_eager = args.enforce_eager
    gpu_memory = args.gpu_memory
    
    if args.preset:
        preset = PRESETS[args.preset]
        quantization = quantization or preset.get("quantization")
        max_model_len = max_model_len or preset.get("max_model_len")
        enforce_eager = enforce_eager or preset.get("enforce_eager", False)
        gpu_memory = preset.get("gpu_memory", gpu_memory)
        if tensor_parallel == 1:
            tensor_parallel = preset.get("tensor_parallel", 1)
    
    start_vllm_server(
        model=args.model,
        host=args.host,
        port=args.port,
        tensor_parallel_size=tensor_parallel,
        gpu_memory_utilization=gpu_memory,
        max_model_len=max_model_len,
        quantization=quantization,
        enforce_eager=enforce_eager,
        dtype=args.dtype,
    )


if __name__ == "__main__":
    main()
