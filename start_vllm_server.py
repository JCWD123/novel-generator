#!/usr/bin/env python3
"""
vLLM æœåŠ¡å¯åŠ¨è„šæœ¬ - æ¨¡ä»¿ Inference-parts-eedi é¡¹ç›®

æ”¯æŒå¼ é‡å¹¶è¡Œéƒ¨ç½²ï¼Œå°†æ¨¡å‹å¹³å‡åˆ†é…åˆ°å¤šå¼  GPUã€‚
è‡ªåŠ¨æ£€æµ‹æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½ã€‚
å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™åŠ é€Ÿã€‚

ç”¨æ³•ï¼š
    # å•å¡éƒ¨ç½²
    python start_vllm_server.py --model 1.5b
    
    # åŒå¡å¼ é‡å¹¶è¡Œï¼ˆæ¨èç”¨äºå¤§æ¨¡å‹ï¼‰
    python start_vllm_server.py --model 70b --tensor-parallel-size 2
    
    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
    python start_vllm_server.py --model /path/to/model --tensor-parallel-size 2
"""

import argparse
import gc
import os
import subprocess
import sys
from pathlib import Path

# ========== ç¯å¢ƒå˜é‡è®¾ç½® ==========
# å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ£€æµ‹ hf_transfer æ˜¯å¦å®‰è£…ï¼Œåªæœ‰å®‰è£…äº†æ‰å¯ç”¨
def _check_hf_transfer():
    try:
        import hf_transfer
        return True
    except ImportError:
        return False

if _check_hf_transfer():
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
    print("âš¡ hf_transfer å·²å®‰è£…ï¼Œå¯ç”¨åŠ é€Ÿä¸‹è½½")
else:
    # å…³é”®ï¼šå¦‚æœæ²¡è£… hf_transferï¼Œå¿…é¡»ç§»é™¤æˆ–è®¾ä¸º 0
    os.environ.pop("HF_HUB_ENABLE_HF_TRANSFER", None)
    os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"
    print("âš ï¸ hf_transfer æœªå®‰è£…ï¼Œä½¿ç”¨æ™®é€šä¸‹è½½æ¨¡å¼")
    print("   æç¤ºï¼špip install hf_transfer å¯åŠ é€Ÿä¸‹è½½")

# æ¨¡å‹é…ç½®
MODELS = {
    "1.5b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
}

# HuggingFace ç¼“å­˜ç›®å½•
HF_CACHE_DIR = os.path.expanduser("~/.cache/huggingface/hub")


def get_local_model_path(model_id: str) -> tuple:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¿”å›æœ¬åœ° snapshot è·¯å¾„
    
    ç›´æ¥æ‰«æ HuggingFace ç¼“å­˜ç›®å½•ç»“æ„
    
    Args:
        model_id: HuggingFace æ¨¡å‹ ID (å¦‚ deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B)
        
    Returns:
        (local_path, status) - status: "complete" / "incomplete" / "not_found"
    """
    # å°†æ¨¡å‹ ID è½¬æ¢ä¸ºç¼“å­˜ç›®å½•åæ ¼å¼
    cache_dir_name = "models--" + model_id.replace("/", "--")
    model_cache_dir = os.path.join(HF_CACHE_DIR, cache_dir_name)
    
    if not os.path.exists(model_cache_dir):
        return None, "not_found"
    
    # æ£€æŸ¥ blobs ç›®å½•æ˜¯å¦æœ‰ .incomplete æ–‡ä»¶
    blobs_dir = os.path.join(model_cache_dir, "blobs")
    if os.path.exists(blobs_dir):
        for f in os.listdir(blobs_dir):
            if f.endswith(".incomplete"):
                return None, "incomplete"
    
    # æŸ¥æ‰¾ snapshots ç›®å½•
    snapshots_dir = os.path.join(model_cache_dir, "snapshots")
    if not os.path.exists(snapshots_dir):
        return None, "not_found"
    
    # éå†æ‰€æœ‰ snapshot
    for snapshot_name in os.listdir(snapshots_dir):
        snapshot_path = os.path.join(snapshots_dir, snapshot_name)
        if not os.path.isdir(snapshot_path):
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ config.json
        config_path = os.path.join(snapshot_path, "config.json")
        if not os.path.exists(config_path):
            continue
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æƒé‡æ–‡ä»¶ï¼ˆå¯èƒ½æ˜¯è½¯é“¾æ¥ï¼‰
        has_model = False
        for f in os.listdir(snapshot_path):
            file_path = os.path.join(snapshot_path, f)
            # æ£€æŸ¥æ–‡ä»¶åæˆ–è½¯é“¾æ¥ç›®æ ‡
            if f.endswith(".safetensors") or f.endswith(".bin"):
                # å¦‚æœæ˜¯è½¯é“¾æ¥ï¼Œæ£€æŸ¥ç›®æ ‡æ˜¯å¦å­˜åœ¨
                if os.path.islink(file_path):
                    target = os.path.realpath(file_path)
                    if os.path.exists(target) and os.path.getsize(target) > 100_000_000:  # > 100MB
                        has_model = True
                        break
                elif os.path.isfile(file_path) and os.path.getsize(file_path) > 100_000_000:
                    has_model = True
                    break
        
        if has_model:
            print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {snapshot_path}")
            return snapshot_path, "complete"
    
    # æœ‰ç¼“å­˜ç›®å½•ä½†æ²¡æœ‰æ¨¡å‹æƒé‡
    return None, "incomplete"


def resolve_model_path(model: str) -> str:
    """
    è§£ææ¨¡å‹è·¯å¾„ - æ ¸å¿ƒé€»è¾‘
    
    å…³é”®ï¼šå¿…é¡»è¿”å›æœ¬åœ°ç»å¯¹è·¯å¾„ç»™ vLLMï¼Œé¿å…è§¦å‘ snapshot_download()
    """
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ä¸”å­˜åœ¨
    if os.path.exists(model):
        abs_path = os.path.abspath(model)
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {abs_path}")
        return abs_path
    
    # è§£æç®€å†™ä¸ºå®Œæ•´æ¨¡å‹ ID
    model_id = MODELS.get(model.lower(), model)
    
    # æŸ¥æ‰¾æœ¬åœ°ç¼“å­˜
    local_path, status = get_local_model_path(model_id)
    
    if status == "complete" and local_path:
        return local_path
    
    # æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å®Œæ•´
    if status == "incomplete":
        print(f"\nâš ï¸ æ¨¡å‹ä¸‹è½½ä¸å®Œæ•´: {model_id}")
        print(f"   å‘ç° .incomplete æ–‡ä»¶ï¼Œè¯´æ˜ä¸‹è½½è¢«ä¸­æ–­")
        print(f"\n   è§£å†³æ–¹æ¡ˆ:")
        print(f"   1. æ¸…é™¤ç¼“å­˜: rm -rf {HF_CACHE_DIR}/models--{model_id.replace('/', '--')}")
        print(f"   2. é‡æ–°ä¸‹è½½: python download_model.py --model {model}")
    else:
        print(f"\nâŒ æ¨¡å‹æœªåœ¨æœ¬åœ°æ‰¾åˆ°: {model_id}")
        print(f"   ç¼“å­˜ç›®å½•: {HF_CACHE_DIR}")
        print(f"\n   è¯·å…ˆä¸‹è½½æ¨¡å‹:")
        print(f"   python download_model.py --model {model}")
    
    print(f"\n   æˆ–è€…æŒ‡å®šæœ¬åœ°è·¯å¾„:")
    print(f"   python start_vllm_server.py --model /path/to/model")
    sys.exit(1)


def start_openai_server(
    model: str,
    host: str = "0.0.0.0",
    port: int = 8000,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.95,
    max_model_len: int = 4096,
    dtype: str = "bfloat16",
    enforce_eager: bool = True,
    quantization: str = None,
    served_model_name: str = "deepseek-r1",
):
    """
    å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨
    """
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
    model_path = resolve_model_path(model)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¯åŠ¨ vLLM OpenAI å…¼å®¹æœåŠ¡å™¨")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"åœ°å€: {host}:{port}")
    print(f"å¼ é‡å¹¶è¡Œ: {tensor_parallel_size} GPU(s)")
    print(f"æ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}")
    print(f"{'='*60}\n")
    
    cmd = [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", model_path,
        "--host", host,
        "--port", str(port),
        "--tensor-parallel-size", str(tensor_parallel_size),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--max-model-len", str(max_model_len),
        "--dtype", dtype,
        "--served-model-name", served_model_name,
        "--trust-remote-code",
    ]
    
    if enforce_eager:
        cmd.append("--enforce-eager")
    
    if quantization:
        cmd.extend(["--quantization", quantization])
    
    print(f"å‘½ä»¤: {' '.join(cmd)}\n")
    
    try:
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æœåŠ¡å™¨å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)


def interactive_generate(
    model: str,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.95,
    max_model_len: int = 4096,
    dtype: str = "bfloat16",
):
    """
    äº¤äº’å¼ç”Ÿæˆ - æ¨¡ä»¿ vllm_generate.py çš„æ–¹å¼
    """
    import torch
    from vllm import LLM, SamplingParams
    
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆå¿…é¡»æ˜¯æœ¬åœ°è·¯å¾„ï¼‰
    model_path = resolve_model_path(model)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ åˆ›å»º vLLM å®ä¾‹")
    print(f"{'='*60}")
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"å¼ é‡å¹¶è¡Œ: {tensor_parallel_size} GPU(s)")
    print(f"{'='*60}\n")
    
    llm = LLM(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        trust_remote_code=True,
        dtype=dtype,
        enforce_eager=True,
        max_model_len=max_model_len,
        disable_log_stats=True,
    )
    
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        repetition_penalty=1.0,
        max_tokens=2048,
    )
    
    print("\n" + "="*60)
    print("ğŸ­ äº¤äº’å¼ç”Ÿæˆæ¨¡å¼")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("="*60 + "\n")
    
    while True:
        try:
            prompt = input(">>> ").strip()
            if prompt.lower() in ["quit", "exit", "q"]:
                break
            if not prompt:
                continue
            
            outputs = llm.generate([prompt], sampling_params)
            generated_text = outputs[0].outputs[0].text
            print(f"\n{generated_text}\n")
            
        except KeyboardInterrupt:
            break
    
    del llm
    torch.cuda.empty_cache()
    gc.collect()
    print("\nâœ… å·²æ¸…ç†")


def main():
    parser = argparse.ArgumentParser(
        description="vLLM æœåŠ¡å™¨ - å¼ é‡å¹¶è¡Œéƒ¨ç½²ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•å¡éƒ¨ç½²ï¼ˆéœ€å…ˆä¸‹è½½æ¨¡å‹ï¼‰
  python download_model.py --model 7b
  python start_vllm_server.py --model 7b
  
  # åŒå¡å¼ é‡å¹¶è¡Œï¼ˆ70B æ¨¡å‹ï¼‰
  python start_vllm_server.py --model 70b --tensor-parallel-size 2
  
  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
  python start_vllm_server.py --model /path/to/model --tensor-parallel-size 2
  
  # ä½¿ç”¨ AWQ é‡åŒ– + åŒå¡
  python start_vllm_server.py --model /path/to/awq-model --quantization awq --tensor-parallel-size 2
  
  # äº¤äº’å¼æ¨¡å¼
  python start_vllm_server.py --model 7b --interactive

æ³¨æ„: 
  - å¿…é¡»å…ˆç”¨ download_model.py ä¸‹è½½æ¨¡å‹
  - ä¼ ç»™ vLLM çš„æ˜¯æœ¬åœ°è·¯å¾„ï¼Œä¸ä¼šè§¦å‘é¢å¤–ä¸‹è½½
        """
    )
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument("--model", "-m", type=str, required=True,
                        help="æ¨¡å‹åç§° (1.5b/7b/14b/32b/70b) æˆ–æœ¬åœ°è·¯å¾„")
    
    # vLLM é…ç½®
    parser.add_argument("--tensor-parallel-size", "--tp", type=int, default=1,
                        help="å¼ é‡å¹¶è¡Œ GPU æ•°é‡ (é»˜è®¤: 1)")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.95,
                        help="GPU æ˜¾å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.95)")
    parser.add_argument("--max-model-len", type=int, default=4096,
                        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 4096)")
    parser.add_argument("--dtype", type=str, default="bfloat16",
                        choices=["bfloat16", "float16", "half"],
                        help="æ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)")
    parser.add_argument("--quantization", "-q", type=str, default=None,
                        choices=["awq", None],
                        help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--enforce-eager", action="store_true", default=True,
                        help="ç¦ç”¨ CUDA å›¾ (é»˜è®¤: True)")
    parser.add_argument("--no-enforce-eager", action="store_false", dest="enforce_eager",
                        help="å¯ç”¨ CUDA å›¾")
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument("--host", type=str, default="0.0.0.0",
                        help="æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--port", "-p", type=int, default=8000,
                        help="æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)")
    parser.add_argument("--served-model-name", type=str, default="deepseek-r1",
                        help="API æ¨¡å‹åç§° (é»˜è®¤: deepseek-r1)")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--interactive", "-i", action="store_true",
                        help="äº¤äº’å¼ç”Ÿæˆæ¨¡å¼")
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_generate(
            model=args.model,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            max_model_len=args.max_model_len,
            dtype=args.dtype,
        )
    else:
        start_openai_server(
            model=args.model,
            host=args.host,
            port=args.port,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            max_model_len=args.max_model_len,
            dtype=args.dtype,
            enforce_eager=args.enforce_eager,
            quantization=args.quantization,
            served_model_name=args.served_model_name,
        )


if __name__ == "__main__":
    main()
