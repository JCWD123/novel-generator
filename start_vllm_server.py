#!/usr/bin/env python3
"""
vLLM æœåŠ¡å¯åŠ¨è„šæœ¬ - æ¨¡ä»¿ Inference-parts-eedi é¡¹ç›®

æ”¯æŒå¼ é‡å¹¶è¡Œéƒ¨ç½²ï¼Œå°†æ¨¡å‹å¹³å‡åˆ†é…åˆ°å¤šå¼  GPUã€‚
è‡ªåŠ¨æ£€æµ‹æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½ã€‚
å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™åŠ é€Ÿã€‚

ç”¨æ³•ï¼š
    # å•å¡éƒ¨ç½²
    python start_vllm_server.py --model 1.5b
    
    # åŒå¡å¼ é‡å¹¶è¡Œï¼ˆæ¨èç”¨äºå¤§æ¨¡å‹ï¼‰
    python start_vllm_server.py --model 70b --tensor-parallel-size 2
    
    # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
    python start_vllm_server.py --model /path/to/model --tensor-parallel-size 2
"""

import argparse
import gc
import os
import subprocess
import sys
from pathlib import Path

# å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ¨¡å‹é…ç½®
MODELS = {
    "1.5b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "7b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
    "14b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "32b": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    "70b": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
}


def get_local_model_path(model_id: str) -> str:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¿”å›æœ¬åœ°è·¯å¾„
    
    Args:
        model_id: HuggingFace æ¨¡å‹ ID
        
    Returns:
        æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å› None
    """
    try:
        from huggingface_hub import scan_cache_dir, try_to_load_from_cache
        
        # æ‰«æç¼“å­˜ç›®å½•
        cache_info = scan_cache_dir()
        
        for repo in cache_info.repos:
            if repo.repo_id == model_id:
                # æ‰¾åˆ°æœ€æ–°çš„ snapshot
                if repo.revisions:
                    for revision in repo.revisions:
                        snapshot_path = revision.snapshot_path
                        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
                        if snapshot_path.exists():
                            model_files = list(snapshot_path.glob("*.safetensors")) + list(snapshot_path.glob("*.bin"))
                            if model_files:
                                print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {snapshot_path}")
                                return str(snapshot_path)
        
        return None
        
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
        return None


def resolve_model_path(model: str) -> str:
    """
    è§£ææ¨¡å‹è·¯å¾„
    
    1. å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„ï¼Œç›´æ¥è¿”å›
    2. å¦‚æœæ˜¯ç®€å†™ï¼ˆå¦‚ 1.5bï¼‰ï¼Œè½¬æ¢ä¸ºå®Œæ•´æ¨¡å‹ ID
    3. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼Œå¦‚æœå·²ä¸‹è½½åˆ™è¿”å›æœ¬åœ°è·¯å¾„
    4. å¦åˆ™è¿”å›æ¨¡å‹ IDï¼ˆvLLM ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰
    """
    # å¦‚æœæ˜¯æœ¬åœ°è·¯å¾„
    if os.path.exists(model):
        print(f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {model}")
        return model
    
    # è§£æç®€å†™
    model_id = MODELS.get(model, model)
    
    # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
    local_path = get_local_model_path(model_id)
    if local_path:
        return local_path
    
    print(f"ğŸ“¥ æ¨¡å‹æœªåœ¨æœ¬åœ°æ‰¾åˆ°ï¼Œå°†é€šè¿‡é•œåƒç«™ä¸‹è½½: {model_id}")
    print(f"ğŸŒ é•œåƒç«™: {os.environ.get('HF_ENDPOINT', 'https://hf-mirror.com')}")
    return model_id


def start_openai_server(
    model: str,
    host: str = "0.0.0.0",
    port: int = 8000,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.95,
    max_model_len: int = 4096,
    dtype: str = "bfloat16",
    enforce_eager: bool = True,
    quantization: str = None,
    served_model_name: str = "deepseek-r1",
):
    """
    å¯åŠ¨ OpenAI å…¼å®¹çš„ API æœåŠ¡å™¨
    """
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
    model_path = resolve_model_path(model)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¯åŠ¨ vLLM OpenAI å…¼å®¹æœåŠ¡å™¨")
    print(f"{'='*60}")
    print(f"æ¨¡å‹: {model_path}")
    print(f"åœ°å€: {host}:{port}")
    print(f"å¼ é‡å¹¶è¡Œ: {tensor_parallel_size} GPU(s)")
    print(f"æ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {max_model_len}")
    print(f"é•œåƒç«™: {os.environ.get('HF_ENDPOINT')}")
    print(f"{'='*60}\n")
    
    cmd = [
        sys.executable, "-m", "vllm.entrypoints.openai.api_server",
        "--model", model_path,
        "--host", host,
        "--port", str(port),
        "--tensor-parallel-size", str(tensor_parallel_size),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--max-model-len", str(max_model_len),
        "--dtype", dtype,
        "--served-model-name", served_model_name,
        "--trust-remote-code",
    ]
    
    if enforce_eager:
        cmd.append("--enforce-eager")
    
    if quantization:
        cmd.extend(["--quantization", quantization])
    
    print(f"å‘½ä»¤: {' '.join(cmd)}\n")
    
    try:
        subprocess.run(cmd, check=True)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æœåŠ¡å™¨å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")


def interactive_generate(
    model: str,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = 0.95,
    max_model_len: int = 4096,
    dtype: str = "bfloat16",
):
    """
    äº¤äº’å¼ç”Ÿæˆ - æ¨¡ä»¿ vllm_generate.py çš„æ–¹å¼
    """
    import torch
    from vllm import LLM, SamplingParams
    
    # è§£ææ¨¡å‹è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰
    model_path = resolve_model_path(model)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ åˆ›å»º vLLM å®ä¾‹")
    print(f"{'='*60}")
    print(f"æ¨¡å‹: {model_path}")
    print(f"å¼ é‡å¹¶è¡Œ: {tensor_parallel_size} GPU(s)")
    print(f"{'='*60}\n")
    
    llm = LLM(
        model=model_path,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory_utilization,
        trust_remote_code=True,
        dtype=dtype,
        enforce_eager=True,
        max_model_len=max_model_len,
        disable_log_stats=True,
    )
    
    sampling_params = SamplingParams(
        temperature=0.7,
        top_p=0.8,
        repetition_penalty=1.0,
        max_tokens=2048,
    )
    
    print("\n" + "="*60)
    print("ğŸ­ äº¤äº’å¼ç”Ÿæˆæ¨¡å¼")
    print("è¾“å…¥ 'quit' é€€å‡º")
    print("="*60 + "\n")
    
    while True:
        try:
            prompt = input(">>> ").strip()
            if prompt.lower() in ["quit", "exit", "q"]:
                break
            if not prompt:
                continue
            
            outputs = llm.generate([prompt], sampling_params)
            generated_text = outputs[0].outputs[0].text
            print(f"\n{generated_text}\n")
            
        except KeyboardInterrupt:
            break
    
    del llm
    torch.cuda.empty_cache()
    gc.collect()
    print("\nâœ… å·²æ¸…ç†")


def main():
    parser = argparse.ArgumentParser(
        description="vLLM æœåŠ¡å™¨ - å¼ é‡å¹¶è¡Œéƒ¨ç½²ï¼ˆå¼ºåˆ¶ä½¿ç”¨é•œåƒç«™ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # å•å¡éƒ¨ç½²
  python start_vllm_server.py --model 7b
  
  # åŒå¡å¼ é‡å¹¶è¡Œï¼ˆ70B æ¨¡å‹ï¼‰
  python start_vllm_server.py --model 70b --tensor-parallel-size 2
  
  # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
  python start_vllm_server.py --model /path/to/model --tensor-parallel-size 2
  
  # ä½¿ç”¨ AWQ é‡åŒ– + åŒå¡
  python start_vllm_server.py --model /path/to/awq-model --quantization awq --tensor-parallel-size 2
  
  # äº¤äº’å¼æ¨¡å¼
  python start_vllm_server.py --model 7b --interactive

æ³¨æ„: 
  - è‡ªåŠ¨æ£€æµ‹æœ¬åœ°å·²ä¸‹è½½çš„æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
  - å¼ºåˆ¶ä½¿ç”¨é•œåƒç«™ (hf-mirror.com) åŠ é€Ÿä¸‹è½½
        """
    )
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument("--model", "-m", type=str, required=True,
                        help="æ¨¡å‹åç§° (1.5b/7b/14b/32b/70b) æˆ–æœ¬åœ°è·¯å¾„")
    
    # vLLM é…ç½®
    parser.add_argument("--tensor-parallel-size", "--tp", type=int, default=1,
                        help="å¼ é‡å¹¶è¡Œ GPU æ•°é‡ (é»˜è®¤: 1)")
    parser.add_argument("--gpu-memory-utilization", type=float, default=0.95,
                        help="GPU æ˜¾å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.95)")
    parser.add_argument("--max-model-len", type=int, default=4096,
                        help="æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 4096)")
    parser.add_argument("--dtype", type=str, default="bfloat16",
                        choices=["bfloat16", "float16", "half"],
                        help="æ•°æ®ç±»å‹ (é»˜è®¤: bfloat16)")
    parser.add_argument("--quantization", "-q", type=str, default=None,
                        choices=["awq", None],
                        help="é‡åŒ–æ–¹å¼")
    parser.add_argument("--enforce-eager", action="store_true", default=True,
                        help="ç¦ç”¨ CUDA å›¾ (é»˜è®¤: True)")
    parser.add_argument("--no-enforce-eager", action="store_false", dest="enforce_eager",
                        help="å¯ç”¨ CUDA å›¾")
    
    # æœåŠ¡å™¨é…ç½®
    parser.add_argument("--host", type=str, default="0.0.0.0",
                        help="æœåŠ¡å™¨åœ°å€ (é»˜è®¤: 0.0.0.0)")
    parser.add_argument("--port", "-p", type=int, default=8000,
                        help="æœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 8000)")
    parser.add_argument("--served-model-name", type=str, default="deepseek-r1",
                        help="API æ¨¡å‹åç§° (é»˜è®¤: deepseek-r1)")
    
    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--interactive", "-i", action="store_true",
                        help="äº¤äº’å¼ç”Ÿæˆæ¨¡å¼")
    
    args = parser.parse_args()
    
    if args.interactive:
        interactive_generate(
            model=args.model,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            max_model_len=args.max_model_len,
            dtype=args.dtype,
        )
    else:
        start_openai_server(
            model=args.model,
            host=args.host,
            port=args.port,
            tensor_parallel_size=args.tensor_parallel_size,
            gpu_memory_utilization=args.gpu_memory_utilization,
            max_model_len=args.max_model_len,
            dtype=args.dtype,
            enforce_eager=args.enforce_eager,
            quantization=args.quantization,
            served_model_name=args.served_model_name,
        )


if __name__ == "__main__":
    main()
