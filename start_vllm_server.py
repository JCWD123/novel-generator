#!/usr/bin/env python3
"""
vLLM æ¨¡å‹æœåŠ¡å¯åŠ¨è„šæœ¬

æ”¯æŒæœ¬åœ° AWQ é‡åŒ–æ¨¡å‹å’Œ HuggingFace åœ¨çº¿æ¨¡å‹ã€‚

ç”¨æ³•ï¼š
    # ä½¿ç”¨æœ¬åœ° AWQ é‡åŒ–æ¨¡å‹ï¼ˆæ¨èï¼‰
    python start_vllm_server.py --preset local-awq-70b
    
    # ä½¿ç”¨è‡ªå®šä¹‰æœ¬åœ°æ¨¡å‹è·¯å¾„
    python start_vllm_server.py --model /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ --quantization awq
    
    # ä½¿ç”¨ HuggingFace åœ¨çº¿æ¨¡å‹
    python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
    
    # 12GB æ˜¾å¡ä¼˜åŒ–é…ç½®
    python start_vllm_server.py --preset 12gb
"""

import os
import sys
import argparse
import subprocess
from pathlib import Path

# ==================== æ¨¡å‹é…ç½® ====================

# æœ¬åœ°æ¨¡å‹è·¯å¾„é…ç½®
LOCAL_MODEL_PATHS = {
    # AWQ é‡åŒ–åçš„æœ¬åœ°æ¨¡å‹
    "deepseek-r1-70b-awq": "/home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ",
    "deepseek-r1-7b-awq": "/home/user/models/DeepSeek-R1-Distill-Qwen-7B-AWQ",
    
    # åŸå§‹æœªé‡åŒ–æ¨¡å‹
    "deepseek-r1-70b": "/home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B",
    "deepseek-r1-7b": "/home/user/models/deepseek-ai--DeepSeek-R1-Distill-Qwen-7B",
}

# HuggingFace åœ¨çº¿æ¨¡å‹é…ç½®
HF_MODEL_CONFIGS = {
    "1.5b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "description": "1.5B å‚æ•°ï¼Œæµ‹è¯•ç”¨",
        "gpu_memory": "4GB+",
    },
    "7b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "description": "7B å‚æ•°ï¼Œå•å¡æ¨è",
        "gpu_memory": "16GB+",
    },
    "14b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "description": "14B å‚æ•°ï¼Œéœ€è¦è¾ƒå¤§æ˜¾å­˜",
        "gpu_memory": "32GB+",
    },
    "32b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "description": "32B å‚æ•°ï¼Œéœ€è¦å¤šå¡",
        "gpu_memory": "64GB+",
    },
    "70b": {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "description": "70B å‚æ•°ï¼Œæœ€é«˜è´¨é‡",
        "gpu_memory": "140GB+",
    },
}

# é¢„è®¾é…ç½®
PRESETS = {
    # ========== æœ¬åœ° AWQ é‡åŒ–æ¨¡å‹é¢„è®¾ ==========
    "local-awq-70b": {
        "description": "æœ¬åœ° AWQ é‡åŒ– 70B æ¨¡å‹ï¼ˆæ¨èï¼Œæ˜¾å­˜å ç”¨çº¦ 35GBï¼‰",
        "model": "/home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ",
        "quantization": "awq",
        "max_model_len": 8192,
        "gpu_memory": 0.90,
        "tensor_parallel": 2,  # 70B å»ºè®®åŒå¡
        "enforce_eager": True,
    },
    "local-awq-70b-single": {
        "description": "æœ¬åœ° AWQ é‡åŒ– 70B æ¨¡å‹ï¼ˆå•å¡æ¨¡å¼ï¼Œéœ€è¦ 48GB+ æ˜¾å­˜ï¼‰",
        "model": "/home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ",
        "quantization": "awq",
        "max_model_len": 4096,
        "gpu_memory": 0.95,
        "tensor_parallel": 1,
        "enforce_eager": True,
    },
    "local-awq-7b": {
        "description": "æœ¬åœ° AWQ é‡åŒ– 7B æ¨¡å‹",
        "model": "/home/user/models/DeepSeek-R1-Distill-Qwen-7B-AWQ",
        "quantization": "awq",
        "max_model_len": 8192,
        "gpu_memory": 0.90,
        "tensor_parallel": 1,
        "enforce_eager": False,
    },
    
    # ========== æ˜¾å­˜ä¼˜åŒ–é¢„è®¾ ==========
    "12gb": {
        "description": "RTX 4080/3080 12GB ä¼˜åŒ–é…ç½®",
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "quantization": None,  # ä½¿ç”¨ BNB æˆ– AWQ
        "max_model_len": 4096,
        "gpu_memory": 0.92,
        "tensor_parallel": 1,
        "enforce_eager": True,
    },
    "24gb": {
        "description": "RTX 4090/A5000 24GB é…ç½®",
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "quantization": "awq",
        "max_model_len": 8192,
        "gpu_memory": 0.90,
        "tensor_parallel": 1,
        "enforce_eager": False,
    },
    "48gb": {
        "description": "A6000/åŒå¡ 48GB é…ç½®",
        "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "quantization": "awq",
        "max_model_len": 16384,
        "gpu_memory": 0.85,
        "tensor_parallel": 1,
        "enforce_eager": False,
    },
    "multi-gpu": {
        "description": "å¤šå¡é…ç½®ï¼ˆè‡ªåŠ¨æ£€æµ‹ GPU æ•°é‡ï¼‰",
        "model": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "quantization": "awq",
        "max_model_len": 8192,
        "gpu_memory": 0.85,
        "tensor_parallel": "auto",
        "enforce_eager": False,
    },
}

# é»˜è®¤é…ç½®
DEFAULT_HOST = "0.0.0.0"
DEFAULT_PORT = 8000
DEFAULT_GPU_MEMORY_UTILIZATION = 0.90


def check_local_model(model_path: str) -> bool:
    """æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ˜¯å¦å­˜åœ¨"""
    path = Path(model_path)
    if path.exists():
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
        model_files = list(path.glob("*.safetensors")) + list(path.glob("*.bin"))
        config_file = path / "config.json"
        if model_files and config_file.exists():
            return True
    return False


def get_gpu_count() -> int:
    """è·å–å¯ç”¨ GPU æ•°é‡"""
    try:
        import torch
        return torch.cuda.device_count()
    except:
        return 1


def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
    print("\n" + "="*70)
    print("ğŸ“¦ å¯ç”¨æ¨¡å‹é…ç½®")
    print("="*70)
    
    print("\nğŸ”¸ æœ¬åœ° AWQ é‡åŒ–æ¨¡å‹:")
    print("-"*70)
    for key, path in LOCAL_MODEL_PATHS.items():
        status = "âœ… å·²å°±ç»ª" if check_local_model(path) else "âŒ æœªæ‰¾åˆ°"
        print(f"  {key}")
        print(f"    è·¯å¾„: {path}")
        print(f"    çŠ¶æ€: {status}")
        print()
    
    print("\nğŸ”¸ HuggingFace åœ¨çº¿æ¨¡å‹:")
    print("-"*70)
    for key, config in HF_MODEL_CONFIGS.items():
        print(f"  {key}")
        print(f"    æ¨¡å‹: {config['name']}")
        print(f"    æ˜¾å­˜: {config['gpu_memory']}")
        print(f"    è¯´æ˜: {config['description']}")
        print()
    
    print("="*70)


def list_presets():
    """åˆ—å‡ºæ‰€æœ‰é¢„è®¾é…ç½®"""
    print("\n" + "="*70)
    print("âš™ï¸ é¢„è®¾é…ç½®")
    print("="*70)
    
    for key, preset in PRESETS.items():
        print(f"\n  --preset {key}")
        print(f"    è¯´æ˜: {preset['description']}")
        print(f"    æ¨¡å‹: {preset['model']}")
        print(f"    é‡åŒ–: {preset.get('quantization') or 'æ— '}")
        print(f"    æœ€å¤§é•¿åº¦: {preset.get('max_model_len', 'é»˜è®¤')}")
        tp = preset.get('tensor_parallel', 1)
        print(f"    å¼ é‡å¹¶è¡Œ: {tp if tp != 'auto' else 'è‡ªåŠ¨æ£€æµ‹'}")
    
    print("\n" + "="*70)


def start_vllm_server(
    model: str,
    host: str = DEFAULT_HOST,
    port: int = DEFAULT_PORT,
    tensor_parallel_size: int = 1,
    gpu_memory_utilization: float = DEFAULT_GPU_MEMORY_UTILIZATION,
    max_model_len: int = None,
    quantization: str = None,
    download_dir: str = None,
    trust_remote_code: bool = True,
    enforce_eager: bool = False,
    dtype: str = None,
    served_model_name: str = "deepseek-r1",
):
    """
    å¯åŠ¨ vLLM OpenAI å…¼å®¹æœåŠ¡å™¨
    """
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    is_local = model.startswith("/") or model.startswith("./")
    if is_local and not check_local_model(model):
        print(f"\nâŒ æœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨: {model}")
        print("\nğŸ’¡ è¯·å…ˆè¿è¡Œé‡åŒ–è„šæœ¬åˆ›å»ºæœ¬åœ°æ¨¡å‹:")
        print(f"   python awq_quantization.py --model_path <åŸå§‹æ¨¡å‹è·¯å¾„> --quant_path {model}")
        print("\n   æˆ–è€…ä½¿ç”¨ HuggingFace åœ¨çº¿æ¨¡å‹:")
        print("   python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
        sys.exit(1)
    
    # æ„å»ºå‘½ä»¤
    cmd = [
        "python", "-m", "vllm.entrypoints.openai.api_server",
        "--model", model,
        "--host", host,
        "--port", str(port),
        "--tensor-parallel-size", str(tensor_parallel_size),
        "--gpu-memory-utilization", str(gpu_memory_utilization),
        "--served-model-name", served_model_name,
    ]
    
    # å¯é€‰å‚æ•°
    if max_model_len:
        cmd.extend(["--max-model-len", str(max_model_len)])
    
    if quantization:
        cmd.extend(["--quantization", quantization])
    
    if download_dir:
        cmd.extend(["--download-dir", download_dir])
    
    if trust_remote_code:
        cmd.append("--trust-remote-code")
    
    if enforce_eager:
        cmd.append("--enforce-eager")
    
    if dtype:
        cmd.extend(["--dtype", dtype])
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸš€ å¯åŠ¨ vLLM æœåŠ¡å™¨")
    print("="*70)
    print(f"ğŸ“¦ æ¨¡å‹: {model}")
    print(f"ğŸ”— æœåŠ¡åœ°å€: http://{host}:{port}")
    print(f"ğŸ¯ API æ¨¡å‹å: {served_model_name}")
    print(f"ğŸ’¾ æ˜¾å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization:.0%}")
    print(f"ğŸ–¥ï¸ GPU æ•°é‡: {tensor_parallel_size}")
    if max_model_len:
        print(f"ğŸ“ æœ€å¤§ä¸Šä¸‹æ–‡: {max_model_len}")
    if quantization:
        print(f"ğŸ”§ é‡åŒ–æ–¹å¼: {quantization.upper()}")
    if enforce_eager:
        print(f"âš¡ CUDA å›¾: ç¦ç”¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
    print("="*70)
    
    print(f"\nğŸ“ æ‰§è¡Œå‘½ä»¤:")
    print(f"   {' '.join(cmd)}\n")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env = os.environ.copy()
    env["TOKENIZERS_PARALLELISM"] = "false"
    env["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
    
    # æ‰§è¡Œå‘½ä»¤
    try:
        subprocess.run(cmd, env=env, check=True)
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æœåŠ¡å™¨å·²åœæ­¢")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        sys.exit(1)
    except FileNotFoundError:
        print("\nâŒ æ‰¾ä¸åˆ° vllm å‘½ä»¤ï¼Œè¯·ç¡®ä¿å·²å®‰è£…:")
        print("   pip install vllm>=0.6.0")
        sys.exit(1)


def main():
    parser = argparse.ArgumentParser(
        description="å¯åŠ¨ vLLM æœåŠ¡å™¨ï¼ˆæ”¯æŒæœ¬åœ° AWQ é‡åŒ–æ¨¡å‹ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“Œ ä½¿ç”¨ç¤ºä¾‹ï¼š

  ğŸ”¹ æ¨èï¼šä½¿ç”¨æœ¬åœ° AWQ é‡åŒ– 70B æ¨¡å‹
     python start_vllm_server.py --preset local-awq-70b

  ğŸ”¹ ä½¿ç”¨è‡ªå®šä¹‰æœ¬åœ°æ¨¡å‹è·¯å¾„
     python start_vllm_server.py \\
         --model /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ \\
         --quantization awq --tp 2

  ğŸ”¹ ä½¿ç”¨ HuggingFace åœ¨çº¿ 7B æ¨¡å‹
     python start_vllm_server.py --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B

  ğŸ”¹ 12GB æ˜¾å¡ä¼˜åŒ–é…ç½®
     python start_vllm_server.py --preset 12gb

  ğŸ”¹ åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹
     python start_vllm_server.py --list-models

  ğŸ”¹ åˆ—å‡ºæ‰€æœ‰é¢„è®¾é…ç½®
     python start_vllm_server.py --list-presets

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ’¡ é‡åŒ–å·¥ä½œæµï¼š
  1. å…ˆè¿è¡Œé‡åŒ–è„šæœ¬ï¼š
     python awq_quantization.py \\
         --model_path /home/user/models/deepseek-ai--DeepSeek-R1-Distill-Llama-70B \\
         --quant_path /home/user/models/DeepSeek-R1-Distill-Llama-70B-AWQ

  2. å¯åŠ¨é‡åŒ–åçš„æ¨¡å‹ï¼š
     python start_vllm_server.py --preset local-awq-70b
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        """
    )
    
    # æ¨¡å‹é€‰æ‹©
    parser.add_argument(
        "--model", "-m",
        type=str,
        default=None,
        help="æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„"
    )
    
    # é¢„è®¾é…ç½®
    parser.add_argument(
        "--preset",
        type=str,
        choices=list(PRESETS.keys()),
        help="ä½¿ç”¨é¢„è®¾é…ç½® (local-awq-70b/12gb/24gb/...)"
    )
    
    # æœåŠ¡é…ç½®
    parser.add_argument(
        "--host",
        type=str,
        default=DEFAULT_HOST,
        help=f"ç›‘å¬åœ°å€ (é»˜è®¤: {DEFAULT_HOST})"
    )
    parser.add_argument(
        "--port", "-p",
        type=int,
        default=DEFAULT_PORT,
        help=f"ç›‘å¬ç«¯å£ (é»˜è®¤: {DEFAULT_PORT})"
    )
    parser.add_argument(
        "--served-model-name",
        type=str,
        default="deepseek-r1",
        help="API æ¨¡å‹åç§° (é»˜è®¤: deepseek-r1)"
    )
    
    # GPU é…ç½®
    parser.add_argument(
        "--tp", "--tensor-parallel-size",
        type=int,
        default=1,
        dest="tensor_parallel_size",
        help="å¼ é‡å¹¶è¡Œ GPU æ•°é‡ (é»˜è®¤: 1)"
    )
    parser.add_argument(
        "--gpu-memory-utilization",
        type=float,
        default=DEFAULT_GPU_MEMORY_UTILIZATION,
        help=f"GPU æ˜¾å­˜åˆ©ç”¨ç‡ (é»˜è®¤: {DEFAULT_GPU_MEMORY_UTILIZATION})"
    )
    
    # æ¨¡å‹é…ç½®
    parser.add_argument(
        "--max-model-len",
        type=int,
        default=None,
        help="æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦"
    )
    parser.add_argument(
        "--quantization", "-q",
        type=str,
        choices=["awq", "gptq", "squeezellm", "fp8", "bitsandbytes"],
        default=None,
        help="é‡åŒ–æ–¹å¼ (awq/gptq/fp8/bitsandbytes)"
    )
    parser.add_argument(
        "--enforce-eager",
        action="store_true",
        help="ç¦ç”¨ CUDA å›¾ï¼Œå‡å°‘æ˜¾å­˜å ç”¨"
    )
    parser.add_argument(
        "--dtype",
        type=str,
        choices=["bfloat16", "float16", "float32", "auto"],
        default=None,
        help="æ•°æ®ç±»å‹"
    )
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument(
        "--download-dir",
        type=str,
        default=None,
        help="æ¨¡å‹ä¸‹è½½ç›®å½•"
    )
    parser.add_argument(
        "--list-models",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"
    )
    parser.add_argument(
        "--list-presets",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰é¢„è®¾é…ç½®"
    )
    
    args = parser.parse_args()
    
    # ä¿¡æ¯æŸ¥è¯¢
    if args.list_models:
        list_models()
        return 0
    
    if args.list_presets:
        list_presets()
        return 0
    
    # åº”ç”¨é¢„è®¾é…ç½®
    if args.preset:
        preset = PRESETS[args.preset]
        print(f"\nğŸ“‹ åº”ç”¨é¢„è®¾é…ç½®: {args.preset}")
        print(f"   {preset['description']}")
        
        # ä½¿ç”¨é¢„è®¾å€¼ï¼ˆä½†ç”¨æˆ·æ˜¾å¼æŒ‡å®šçš„å‚æ•°ä¼˜å…ˆï¼‰
        model = args.model or preset.get("model")
        quantization = args.quantization or preset.get("quantization")
        max_model_len = args.max_model_len or preset.get("max_model_len")
        enforce_eager = args.enforce_eager or preset.get("enforce_eager", False)
        
        # å¤„ç†å¼ é‡å¹¶è¡Œ
        tp = preset.get("tensor_parallel", 1)
        if tp == "auto":
            tp = get_gpu_count()
            print(f"   è‡ªåŠ¨æ£€æµ‹åˆ° {tp} ä¸ª GPU")
        tensor_parallel_size = args.tensor_parallel_size if args.tensor_parallel_size != 1 else tp
        
        gpu_memory = preset.get("gpu_memory", DEFAULT_GPU_MEMORY_UTILIZATION)
        if args.gpu_memory_utilization != DEFAULT_GPU_MEMORY_UTILIZATION:
            gpu_memory = args.gpu_memory_utilization
    else:
        # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é»˜è®¤å€¼
        model = args.model or "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
        quantization = args.quantization
        max_model_len = args.max_model_len
        enforce_eager = args.enforce_eager
        tensor_parallel_size = args.tensor_parallel_size
        gpu_memory = args.gpu_memory_utilization
    
    # å¯åŠ¨æœåŠ¡å™¨
    start_vllm_server(
        model=model,
        host=args.host,
        port=args.port,
        tensor_parallel_size=tensor_parallel_size,
        gpu_memory_utilization=gpu_memory,
        max_model_len=max_model_len,
        quantization=quantization,
        download_dir=args.download_dir,
        enforce_eager=enforce_eager,
        dtype=args.dtype,
        served_model_name=args.served_model_name,
    )


if __name__ == "__main__":
    main()
