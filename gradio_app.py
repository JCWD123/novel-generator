#!/usr/bin/env python3
"""
å°è¯´ç”Ÿæˆå™¨ Gradio å‰ç«¯
ç›´æ¥ä½¿ç”¨ transformers åŠ è½½ AWQ é‡åŒ–æ¨¡å‹ï¼ˆä¸ä¾èµ– vLLMï¼‰

ç”¨æ³•:
    python gradio_app.py --model_path /path/to/awq_model
    
    æˆ–ä½¿ç”¨ç¯å¢ƒå˜é‡:
    MODEL_PATH=/path/to/awq_model python gradio_app.py
"""

import gradio as gr
import torch
import os
import json
import re
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Generator, Optional, Tuple
from threading import Thread

# LangChain æ¶ˆæ¯ç±»å‹ï¼ˆè½»é‡çº§å®ç°ï¼Œä¸ä¾èµ–å®Œæ•´ langchainï¼‰
class BaseMessage:
    """æ¶ˆæ¯åŸºç±»"""
    def __init__(self, content: str, role: str = ""):
        self.content = content
        self.role = role
    
    def to_dict(self) -> Dict[str, str]:
        return {"role": self.role, "content": self.content}
    
    @classmethod
    def from_dict(cls, data: Dict) -> "BaseMessage":
        role = data.get("role", "")
        content = data.get("content", "")
        if role == "user" or role == "human":
            return HumanMessage(content)
        elif role == "assistant" or role == "ai":
            return AIMessage(content)
        elif role == "system":
            return SystemMessage(content)
        return BaseMessage(content, role)

class HumanMessage(BaseMessage):
    """ç”¨æˆ·æ¶ˆæ¯"""
    def __init__(self, content: str):
        super().__init__(content, "user")

class AIMessage(BaseMessage):
    """AI åŠ©æ‰‹æ¶ˆæ¯"""
    def __init__(self, content: str):
        super().__init__(content, "assistant")

class SystemMessage(BaseMessage):
    """ç³»ç»Ÿæ¶ˆæ¯"""
    def __init__(self, content: str):
        super().__init__(content, "system")


class ChatHistory:
    """èŠå¤©å†å²ç®¡ç†å™¨ - ä½¿ç”¨ LangChain é£æ ¼çš„æ¶ˆæ¯å¯¹è±¡"""
    
    def __init__(self):
        self.messages: List[BaseMessage] = []
    
    def add_user_message(self, content: str):
        """æ·»åŠ ç”¨æˆ·æ¶ˆæ¯"""
        self.messages.append(HumanMessage(content))
    
    def add_ai_message(self, content: str):
        """æ·»åŠ  AI æ¶ˆæ¯"""
        self.messages.append(AIMessage(content))
    
    def update_last_ai_message(self, content: str):
        """æ›´æ–°æœ€åä¸€æ¡ AI æ¶ˆæ¯ï¼ˆç”¨äºæµå¼ç”Ÿæˆï¼‰"""
        if self.messages and isinstance(self.messages[-1], AIMessage):
            self.messages[-1].content = content
    
    def get_messages_for_model(self) -> List[Dict[str, str]]:
        """è·å–ç”¨äºæ¨¡å‹çš„æ¶ˆæ¯åˆ—è¡¨"""
        return [msg.to_dict() for msg in self.messages]
    
    def to_gradio_format(self) -> List[Dict[str, str]]:
        """è½¬æ¢ä¸º Gradio 6.x Chatbot æ ¼å¼ [{"role": "...", "content": "..."}, ...]"""
        return [{"role": msg.role, "content": msg.content} for msg in self.messages]
    
    @classmethod
    def from_gradio_format(cls, messages: List[Dict[str, str]]) -> "ChatHistory":
        """ä» Gradio 6.x Chatbot æ ¼å¼åˆ›å»º"""
        history = cls()
        for msg in messages:
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                history.add_user_message(content)
            elif role == "assistant":
                history.add_ai_message(content)
        return history
    
    def to_json_serializable(self) -> List[Dict]:
        """è½¬æ¢ä¸ºå¯ JSON åºåˆ—åŒ–çš„æ ¼å¼"""
        return [msg.to_dict() for msg in self.messages]
    
    @classmethod
    def from_json(cls, data: List[Dict]) -> "ChatHistory":
        """ä» JSON æ•°æ®æ¢å¤"""
        history = cls()
        for item in data:
            history.messages.append(BaseMessage.from_dict(item))
        return history
    
    def clear(self):
        """æ¸…ç©ºå†å²"""
        self.messages = []
    
    def __len__(self):
        return len(self.messages)
    
    def __bool__(self):
        return len(self.messages) > 0

# ==================== é…ç½® ====================
MODEL_PATH = os.getenv("MODEL_PATH", "./models/DeepSeek-R1-AWQ")
HISTORY_DIR = Path(__file__).parent / "chat_history"
HISTORY_DIR.mkdir(exist_ok=True)

# æ£€æµ‹è®¾å¤‡
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸ è¿è¡Œè®¾å¤‡: {DEVICE}")

# å°è¯´åˆ›ä½œç³»ç»Ÿæç¤ºè¯
NOVEL_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½æ‰åæ¨ªæº¢çš„å°è¯´ä½œå®¶ï¼Œæ“…é•¿åˆ›ä½œå¼•äººå…¥èƒœçš„æ•…äº‹ã€‚ä½ çš„å†™ä½œç‰¹ç‚¹ï¼š

1. **æ–‡ç¬”ä¼˜ç¾**: å–„äºè¿ç”¨ä¿®è¾æ‰‹æ³•ï¼Œæ–‡å­—å¯Œæœ‰è¯—æ„å’Œç”»é¢æ„Ÿ
2. **æƒ…èŠ‚ç´§å‡‘**: æ•…äº‹å‘å±•æœ‰å¼ æœ‰å¼›ï¼Œæƒ…èŠ‚è·Œå®•èµ·ä¼
3. **äººç‰©é²œæ˜**: è§’è‰²æ€§æ ¼ç«‹ä½“ï¼Œå¯¹è¯ç”ŸåŠ¨è‡ªç„¶
4. **ç»†èŠ‚ä¸°å¯Œ**: åœºæ™¯æå†™ç»†è…»ï¼Œèƒ½è®©è¯»è€…èº«ä¸´å…¶å¢ƒ
5. **è¿è´¯æ€§å¼º**: èƒ½å¤Ÿæ ¹æ®ä¹‹å‰çš„æƒ…èŠ‚è‡ªç„¶å»¶ç»­æ•…äº‹å‘å±•

è¯·æ ¹æ®ç”¨æˆ·çš„è¦æ±‚è¿›è¡Œå°è¯´åˆ›ä½œã€‚åœ¨ç»­å†™æ—¶ï¼Œè¦ä¿æŒä¸ä¹‹å‰å†…å®¹çš„é£æ ¼ä¸€è‡´å’Œæƒ…èŠ‚è¿è´¯ã€‚
è¾“å‡ºæ ¼å¼è¦æ±‚ï¼šç›´æ¥è¾“å‡ºå°è¯´å†…å®¹ï¼Œä¸è¦ä½¿ç”¨markdownæ ¼å¼ï¼Œä¸è¦æ·»åŠ é¢å¤–çš„è§£é‡Šã€‚"""


# ==================== å…¨å±€æ¨¡å‹å˜é‡ ====================
model = None
tokenizer = None


def load_awq_model(model_path: str):
    """
    åŠ è½½ AWQ é‡åŒ–æ¨¡å‹
    æ”¯æŒä¸¤ç§æ–¹å¼ï¼š
    1. ä½¿ç”¨ transformers åŸç”ŸåŠ è½½ï¼ˆæ¨èï¼‰
    2. ä½¿ç”¨ AutoAWQ åŠ è½½
    """
    global model, tokenizer
    
    print(f"\n{'='*60}")
    print(f"ğŸ“¥ åŠ è½½ AWQ æ¨¡å‹: {model_path}")
    print(f"{'='*60}")
    
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # æ£€æŸ¥æ˜¯å¦ä¸º AWQ æ¨¡å‹
    config_path = Path(model_path) / "config.json"
    is_awq = False
    
    if config_path.exists():
        with open(config_path, "r") as f:
            config = json.load(f)
            if "quantization_config" in config:
                quant_config = config["quantization_config"]
                if quant_config.get("quant_method") == "awq":
                    is_awq = True
                    print(f"âœ… æ£€æµ‹åˆ° AWQ é‡åŒ–é…ç½®:")
                    print(f"   - bits: {quant_config.get('bits', 4)}")
                    print(f"   - group_size: {quant_config.get('group_size', 128)}")
    
    # åŠ è½½ tokenizer
    print("ğŸ“ åŠ è½½ Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, 
        trust_remote_code=True,
        use_fast=True
    )
    
    # è®¾ç½® pad_token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    print("ğŸ”§ åŠ è½½æ¨¡å‹æƒé‡...")
    
    # å¯¹äº AWQ æ¨¡å‹ï¼Œtransformers 4.36+ åŸç”Ÿæ”¯æŒ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨è®¾å¤‡
        trust_remote_code=True,
        low_cpu_mem_usage=True,
    )
    
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    
    print(f"\n{'='*60}")
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    print(f"   - è®¾å¤‡: {next(model.parameters()).device}")
    print(f"   - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
    print(f"{'='*60}\n")
    
    return model, tokenizer


def format_messages(messages: List[Dict], system_prompt: str = NOVEL_SYSTEM_PROMPT) -> str:
    """
    å°†å¯¹è¯æ¶ˆæ¯æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥
    æ”¯æŒå¤šç§å¯¹è¯æ¨¡æ¿æ ¼å¼
    """
    # æ„å»ºå®Œæ•´çš„æ¶ˆæ¯åˆ—è¡¨
    full_messages = [{"role": "system", "content": system_prompt}] + messages
    
    # å°è¯•ä½¿ç”¨ tokenizer çš„ chat æ¨¡æ¿
    if hasattr(tokenizer, "apply_chat_template"):
        try:
            prompt = tokenizer.apply_chat_template(
                full_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            return prompt
        except Exception as e:
            print(f"âš ï¸ apply_chat_template å¤±è´¥: {e}")
    
    # å›é€€åˆ°æ‰‹åŠ¨æ„å»º (ChatML æ ¼å¼)
    prompt = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        prompt += f"<|im_start|>{role}\n{content}<|im_end|>\n"
    prompt += "<|im_start|>assistant\n"
    
    return prompt


@torch.inference_mode()
def generate_response(
    messages: List[Dict],
    max_new_tokens: int = 2048,
    temperature: float = 0.8,
    top_p: float = 0.95,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
) -> str:
    """
    ç”Ÿæˆæ¨¡å‹å“åº”ï¼ˆéæµå¼ï¼‰
    """
    if model is None or tokenizer is None:
        return "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
    
    # æ¸…ç† GPU ç¼“å­˜
    torch.cuda.empty_cache()
    
    # æ ¼å¼åŒ–è¾“å…¥
    prompt = format_messages(messages)
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    ).to(model.device)
    
    # ç”Ÿæˆ
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        repetition_penalty=repetition_penalty,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    
    # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    response = tokenizer.decode(
        outputs[0][inputs["input_ids"].shape[1]:],
        skip_special_tokens=True
    )
    
    return response.strip()


@torch.inference_mode()
def generate_response_stream(
    messages: List[Dict],
    max_new_tokens: int = 2048,
    temperature: float = 0.8,
    top_p: float = 0.95,
    top_k: int = 50,
    repetition_penalty: float = 1.1,
) -> Generator[str, None, None]:
    """
    ç”Ÿæˆæ¨¡å‹å“åº”ï¼ˆæµå¼ï¼‰
    """
    if model is None or tokenizer is None:
        yield "âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆåŠ è½½æ¨¡å‹"
        return
    
    from transformers import TextIteratorStreamer
    
    # æ¸…ç† GPU ç¼“å­˜
    torch.cuda.empty_cache()
    
    # æ ¼å¼åŒ–è¾“å…¥
    prompt = format_messages(messages)
    
    # Tokenize
    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=4096
    ).to(model.device)
    
    # åˆ›å»ºæµå¼è¾“å‡ºå™¨
    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
    )
    
    # åœ¨åå°çº¿ç¨‹ä¸­ç”Ÿæˆ
    generation_kwargs = {
        "input_ids": inputs["input_ids"],
        "attention_mask": inputs["attention_mask"],
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "top_k": top_k,
        "repetition_penalty": repetition_penalty,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "streamer": streamer,
    }
    
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # æµå¼è¾“å‡º
    generated_text = ""
    for new_text in streamer:
        generated_text += new_text
        yield generated_text
    
    thread.join()


# ==================== å†å²è®°å½•ç®¡ç† ====================
def get_history_files() -> List[Dict]:
    """è·å–æ‰€æœ‰å†å²è®°å½•æ–‡ä»¶"""
    files = []
    for f in sorted(HISTORY_DIR.glob("*.json"), key=os.path.getmtime, reverse=True):
        try:
            with open(f, "r", encoding="utf-8") as fp:
                data = json.load(fp)
                files.append({
                    "path": str(f),
                    "name": data.get("title", f.stem),
                    "modified": datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M"),
                    "messages": data.get("messages", [])
                })
        except:
            continue
    return files


def save_history(title: str, chat_history_gradio: List[Dict[str, str]]):
    """ä¿å­˜å†å²å¯¹è¯"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    safe_title = re.sub(r'[\\/*?:"<>|]', "", title)[:50]
    filename = f"{timestamp}_{safe_title}.json"
    filepath = HISTORY_DIR / filename
    
    # ä½¿ç”¨ ChatHistory è½¬æ¢æ ¼å¼
    history = ChatHistory.from_gradio_format(chat_history_gradio)
    
    data = {
        "title": title,
        "created": datetime.now().isoformat(),
        "messages": history.to_json_serializable()
    }
    
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return str(filepath)


def load_history_file(filepath: str) -> Tuple[str, List[Dict[str, str]]]:
    """åŠ è½½å†å²è®°å½•"""
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    title = data.get("title", "æœªå‘½å")
    messages = data.get("messages", [])
    
    # ä½¿ç”¨ ChatHistory è½¬æ¢ä¸º Gradio æ ¼å¼
    history = ChatHistory.from_json(messages)
    return title, history.to_gradio_format()


def export_novel(chat_history_gradio: List[Dict[str, str]], title: str) -> str:
    """å¯¼å‡ºå°è¯´ä¸ºçº¯æ–‡æœ¬"""
    lines = [f"ã€Š{title}ã€‹\n", "="*50 + "\n\n"]
    
    for msg in chat_history_gradio:
        if msg.get("role") == "assistant":  # åªå¯¼å‡º AI ç”Ÿæˆçš„å†…å®¹
            content = msg.get("content", "")
            if content:
                lines.append(content)
                lines.append("\n\n")
    
    return "".join(lines)


# ==================== Gradio ç•Œé¢ ====================
def create_gradio_interface():
    """åˆ›å»º Gradio ç•Œé¢"""
    
    # è‡ªå®šä¹‰ CSS
    custom_css = """
    @import url('https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;600;700&family=Ma+Shan+Zheng&display=swap');
    
    .gradio-container {
        max-width: 1200px !important;
        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%) !important;
    }
    
    .main-title {
        font-family: 'Ma Shan Zheng', cursive !important;
        font-size: 3rem !important;
        text-align: center !important;
        background: linear-gradient(90deg, #e94560, #f39c12, #e94560) !important;
        background-size: 200% auto !important;
        -webkit-background-clip: text !important;
        -webkit-text-fill-color: transparent !important;
        animation: shine 3s linear infinite !important;
        margin-bottom: 0.5rem !important;
    }
    
    @keyframes shine {
        to { background-position: 200% center; }
    }
    
    .subtitle {
        font-family: 'Noto Serif SC', serif !important;
        text-align: center !important;
        color: #a0a0a0 !important;
        font-size: 1.1rem !important;
        margin-bottom: 2rem !important;
    }
    
    #chatbot {
        font-family: 'Noto Serif SC', serif !important;
        min-height: 500px !important;
    }
    
    #chatbot .message {
        font-size: 1.1rem !important;
        line-height: 1.8 !important;
    }
    
    .user-message {
        background: rgba(233, 69, 96, 0.1) !important;
        border-left: 4px solid #f39c12 !important;
    }
    
    .bot-message {
        background: rgba(255, 255, 255, 0.05) !important;
        border-left: 4px solid #e94560 !important;
    }
    
    .generate-btn {
        background: linear-gradient(135deg, #e94560, #f39c12) !important;
        color: white !important;
        font-weight: 600 !important;
        border: none !important;
        border-radius: 8px !important;
    }
    
    .generate-btn:hover {
        transform: scale(1.02) !important;
        box-shadow: 0 4px 15px rgba(233, 69, 96, 0.4) !important;
    }
    
    .parameter-slider label {
        color: #e0e0e0 !important;
    }
    """
    
    with gr.Blocks(
        title="ğŸ“š AI å°è¯´ç”Ÿæˆå™¨",
    ) as demo:
        
        # çŠ¶æ€å˜é‡
        current_title = gr.State("")
        
        # æ ‡é¢˜
        gr.HTML("""
        <h1 class="main-title">ğŸ“š AI å°è¯´ç”Ÿæˆå™¨</h1>
        <p class="subtitle">åŸºäº AWQ é‡åŒ–æ¨¡å‹çš„æ™ºèƒ½å°è¯´åˆ›ä½œåŠ©æ‰‹ | ä½¿ç”¨ Transformers ç›´æ¥éƒ¨ç½²</p>
        """)
        
        with gr.Row():
            # å·¦ä¾§ï¼šä¸»èŠå¤©åŒºåŸŸ
            with gr.Column(scale=3):
                # å°è¯´æ ‡é¢˜
                novel_title = gr.Textbox(
                    label="ğŸ“– å°è¯´æ ‡é¢˜",
                    placeholder="è¾“å…¥ä½ çš„å°è¯´æ ‡é¢˜...",
                    lines=1
                )
                
                # èŠå¤©ç•Œé¢
                chatbot = gr.Chatbot(
                    label="åˆ›ä½œåŒº",
                    elem_id="chatbot",
                    height=500,
                )
                
                # è¾“å…¥åŒºåŸŸ
                with gr.Row():
                    user_input = gr.Textbox(
                        label="åˆ›ä½œæŒ‡ä»¤",
                        placeholder="æè¿°ä½ æƒ³è¦çš„æ•…äº‹æƒ…èŠ‚ã€äººç‰©ã€åœºæ™¯ï¼Œæˆ–è€…è¾“å…¥'ç»§ç»­'è®© AI ç»­å†™...",
                        lines=3,
                        scale=5
                    )
                
                with gr.Row():
                    generate_btn = gr.Button(
                        "âœï¸ ç”Ÿæˆ", 
                        variant="primary",
                        elem_classes="generate-btn",
                        scale=2
                    )
                    continue_btn = gr.Button(
                        "â© ç»­å†™",
                        scale=1
                    )
                    clear_btn = gr.Button(
                        "ğŸ—‘ï¸ æ¸…ç©º",
                        scale=1
                    )
            
            # å³ä¾§ï¼šå‚æ•°å’Œå†å²
            with gr.Column(scale=1):
                # æ¨¡å‹çŠ¶æ€
                with gr.Accordion("ğŸ”§ æ¨¡å‹çŠ¶æ€", open=True):
                    model_status = gr.Textbox(
                        label="çŠ¶æ€",
                        value="ç­‰å¾…åŠ è½½æ¨¡å‹...",
                        interactive=False,
                        lines=2
                    )
                    model_path_input = gr.Textbox(
                        label="æ¨¡å‹è·¯å¾„",
                        value=MODEL_PATH,
                        placeholder="è¾“å…¥ AWQ æ¨¡å‹è·¯å¾„"
                    )
                    load_model_btn = gr.Button("ğŸ“¥ åŠ è½½æ¨¡å‹", variant="secondary")
                
                # ç”Ÿæˆå‚æ•°
                with gr.Accordion("âš™ï¸ ç”Ÿæˆå‚æ•°", open=True):
                    max_tokens = gr.Slider(
                        label="æœ€å¤§ç”Ÿæˆé•¿åº¦",
                        minimum=256,
                        maximum=4096,
                        value=2048,
                        step=128,
                        elem_classes="parameter-slider"
                    )
                    temperature = gr.Slider(
                        label="åˆ›æ„ç¨‹åº¦ (Temperature)",
                        minimum=0.1,
                        maximum=1.5,
                        value=0.8,
                        step=0.1
                    )
                    top_p = gr.Slider(
                        label="Top-P",
                        minimum=0.1,
                        maximum=1.0,
                        value=0.95,
                        step=0.05
                    )
                    repetition_penalty = gr.Slider(
                        label="é‡å¤æƒ©ç½š",
                        minimum=1.0,
                        maximum=2.0,
                        value=1.1,
                        step=0.1
                    )
                
                # å†å²è®°å½•
                with gr.Accordion("ğŸ“š å†å²ä½œå“", open=False):
                    history_dropdown = gr.Dropdown(
                        label="é€‰æ‹©å†å²è®°å½•",
                        choices=[],
                        interactive=True
                    )
                    refresh_history_btn = gr.Button("ğŸ”„ åˆ·æ–°")
                    load_history_btn = gr.Button("ğŸ“‚ åŠ è½½")
                
                # å¯¼å‡º
                with gr.Accordion("ğŸ“¥ å¯¼å‡º", open=False):
                    save_btn = gr.Button("ğŸ’¾ ä¿å­˜å½“å‰å¯¹è¯")
                    export_btn = gr.Button("ğŸ“„ å¯¼å‡ºä¸ºTXT")
                    export_output = gr.File(label="ä¸‹è½½æ–‡ä»¶")
        
        # ==================== äº‹ä»¶å¤„ç† ====================
        
        def on_load_model(path):
            """åŠ è½½æ¨¡å‹"""
            try:
                load_awq_model(path)
                return f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\nè®¾å¤‡: {DEVICE}"
            except Exception as e:
                return f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        
        def on_generate(user_msg, chat_history_gradio, title, max_tok, temp, top_p_val, rep_penalty):
            """ç”Ÿæˆå“åº”"""
            import sys
            print(f"\nğŸ“ [on_generate] æ”¶åˆ°è¯·æ±‚:", flush=True)
            print(f"   - ç”¨æˆ·æ¶ˆæ¯: {user_msg[:50]}..." if len(user_msg) > 50 else f"   - ç”¨æˆ·æ¶ˆæ¯: {user_msg}", flush=True)
            print(f"   - å½“å‰å†å²é•¿åº¦: {len(chat_history_gradio)}", flush=True)
            print(f"   - æ¨¡å‹çŠ¶æ€: {'å·²åŠ è½½' if model is not None else 'æœªåŠ è½½'}", flush=True)
            sys.stdout.flush()
            
            if not user_msg.strip():
                print("   âš ï¸ ç”¨æˆ·æ¶ˆæ¯ä¸ºç©ºï¼Œè·³è¿‡ç”Ÿæˆ")
                return chat_history_gradio, ""
            
            if model is None:
                print("   âŒ æ¨¡å‹æœªåŠ è½½")
                chat_history_gradio.append({"role": "user", "content": user_msg})
                chat_history_gradio.append({"role": "assistant", "content": "âŒ è¯·å…ˆåŠ è½½æ¨¡å‹"})
                return chat_history_gradio, ""
            
            # ä½¿ç”¨ ChatHistory ç®¡ç†æ¶ˆæ¯
            history = ChatHistory.from_gradio_format(chat_history_gradio)
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            history.add_user_message(user_msg)
            
            # è·å–ç”¨äºæ¨¡å‹çš„æ¶ˆæ¯æ ¼å¼
            messages = history.get_messages_for_model()
            print(f"   - å‘é€ç»™æ¨¡å‹çš„æ¶ˆæ¯æ•°: {len(messages)}")
            
            # æ·»åŠ ç©ºçš„ AI æ¶ˆæ¯å ä½
            history.add_ai_message("")
            
            # æµå¼ç”Ÿæˆ
            print("   ğŸš€ å¼€å§‹æµå¼ç”Ÿæˆ...")
            token_count = 0
            for response in generate_response_stream(
                messages,
                max_new_tokens=max_tok,
                temperature=temp,
                top_p=top_p_val,
                repetition_penalty=rep_penalty
            ):
                token_count += 1
                history.update_last_ai_message(response)
                yield history.to_gradio_format(), ""
            
            print(f"   âœ… ç”Ÿæˆå®Œæˆï¼Œå…± {token_count} æ¬¡æ›´æ–°")
        
        def on_continue(chat_history_gradio, title, max_tok, temp, top_p_val, rep_penalty):
            """ç»­å†™"""
            import sys
            print(f"\nâ© [on_continue] ç»­å†™è¯·æ±‚", flush=True)
            print(f"   - å½“å‰å†å²é•¿åº¦: {len(chat_history_gradio)}", flush=True)
            sys.stdout.flush()
            
            if not chat_history_gradio:
                print("   âš ï¸ å†å²ä¸ºç©ºï¼Œæ— æ³•ç»­å†™")
                # è¿”å›æç¤ºæ¶ˆæ¯
                yield [{"role": "assistant", "content": "âš ï¸ è¯·å…ˆè¾“å…¥å†…å®¹å¼€å§‹åˆ›ä½œï¼Œå†ç‚¹å‡»ç»­å†™"}], ""
                return
            
            continue_msg = "è¯·ç»§ç»­åˆ›ä½œï¼Œå»¶ç»­ä¸Šæ–‡çš„æ•…äº‹æƒ…èŠ‚ã€‚"
            
            # ä½¿ç”¨ç”Ÿæˆå™¨
            for result in on_generate(
                continue_msg, chat_history_gradio, title, 
                max_tok, temp, top_p_val, rep_penalty
            ):
                yield result
        
        def on_clear():
            """æ¸…ç©ºå¯¹è¯"""
            return [], ""
        
        def on_refresh_history():
            """åˆ·æ–°å†å²è®°å½•åˆ—è¡¨"""
            files = get_history_files()
            choices = [(f"{f['name']} ({f['modified']})", f['path']) for f in files]
            return gr.Dropdown(choices=choices)
        
        def on_load_history(selected):
            """åŠ è½½é€‰ä¸­çš„å†å²è®°å½•"""
            if not selected:
                return [], ""
            title, messages = load_history_file(selected)
            return messages, title
        
        def on_save(chat_history, title):
            """ä¿å­˜å¯¹è¯"""
            if not chat_history:
                return "æ²¡æœ‰å¯ä¿å­˜çš„å¯¹è¯"
            if not title:
                title = "æœªå‘½åå°è¯´"
            filepath = save_history(title, chat_history)
            return f"âœ… å·²ä¿å­˜åˆ°: {filepath}"
        
        def on_export(chat_history, title):
            """å¯¼å‡ºä¸ºTXT"""
            if not chat_history:
                return None
            if not title:
                title = "æœªå‘½åå°è¯´"
            
            content = export_novel(chat_history, title)
            
            # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
            export_path = HISTORY_DIR / f"{title}_export.txt"
            with open(export_path, "w", encoding="utf-8") as f:
                f.write(content)
            
            return str(export_path)
        
        # ç»‘å®šäº‹ä»¶
        load_model_btn.click(
            on_load_model,
            inputs=[model_path_input],
            outputs=[model_status]
        )
        
        generate_btn.click(
            on_generate,
            inputs=[user_input, chatbot, novel_title, max_tokens, temperature, top_p, repetition_penalty],
            outputs=[chatbot, user_input]
        )
        
        # Enter é”®è§¦å‘ç”Ÿæˆ
        user_input.submit(
            on_generate,
            inputs=[user_input, chatbot, novel_title, max_tokens, temperature, top_p, repetition_penalty],
            outputs=[chatbot, user_input]
        )
        
        continue_btn.click(
            on_continue,
            inputs=[chatbot, novel_title, max_tokens, temperature, top_p, repetition_penalty],
            outputs=[chatbot, user_input]
        )
        
        clear_btn.click(
            on_clear,
            outputs=[chatbot, user_input]
        )
        
        refresh_history_btn.click(
            on_refresh_history,
            outputs=[history_dropdown]
        )
        
        load_history_btn.click(
            on_load_history,
            inputs=[history_dropdown],
            outputs=[chatbot, novel_title]
        )
        
        save_btn.click(
            on_save,
            inputs=[chatbot, novel_title],
            outputs=[model_status]
        )
        
        export_btn.click(
            on_export,
            inputs=[chatbot, novel_title],
            outputs=[export_output]
        )
        
        # å¯åŠ¨æ—¶åˆ·æ–°å†å²è®°å½•
        demo.load(on_refresh_history, outputs=[history_dropdown])
    
    return demo


# ==================== ä¸»ç¨‹åº ====================
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="å°è¯´ç”Ÿæˆå™¨ Gradio å‰ç«¯")
    parser.add_argument("--model_path", type=str, default=MODEL_PATH, help="AWQ æ¨¡å‹è·¯å¾„")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=7860, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--share", action="store_true", help="åˆ›å»ºå…¬å…±é“¾æ¥")
    parser.add_argument("--auto_load", action="store_true", help="å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹")
    args = parser.parse_args()
    
    # æ›´æ–°æ¨¡å‹è·¯å¾„
    MODEL_PATH = args.model_path
    
    # è‡ªåŠ¨åŠ è½½æ¨¡å‹
    if args.auto_load:
        print("ğŸš€ è‡ªåŠ¨åŠ è½½æ¨¡å‹...")
        try:
            load_awq_model(args.model_path)
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ å¯ä»¥åœ¨ç•Œé¢ä¸­æ‰‹åŠ¨åŠ è½½æ¨¡å‹")
    
    # åˆ›å»ºå¹¶å¯åŠ¨ Gradio ç•Œé¢
    demo = create_gradio_interface()
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¯åŠ¨ Gradio æœåŠ¡å™¨")
    print(f"{'='*60}")
    print(f"   åœ°å€: http://{args.host}:{args.port}")
    if args.share:
        print(f"   å…¬å…±é“¾æ¥: ç”Ÿæˆä¸­...")
    print(f"{'='*60}\n")
    
    demo.launch(
        server_name=args.host,
        server_port=args.port,
        share=args.share,
        show_error=True
    )
